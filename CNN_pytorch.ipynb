{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, QuantileTransformer, MinMaxScaler\n",
    "\n",
    "from umap import UMAP\n",
    "from sklearn.cluster import DBSCAN\n",
    "import time\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add labels to meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_raw = [np.load('./labels/'+x) for x in os.listdir('./labels/') if x.endswith('.npy')]\n",
    "\n",
    "label_dict = {}\n",
    "value_array = []\n",
    "for values in labels_raw:\n",
    "    for value,ind in zip(values[0],values[1]):\n",
    "        value_array.append(value)\n",
    "        #print(value)\n",
    "        label_dict[value] = ind\n",
    "\n",
    "meta_data = pd.read_csv('cluster_meta.csv')\n",
    "file_index = meta_data['cluster'].values\n",
    "\n",
    "label_list = [label_dict[int(x[:-2])] for x in file_index]\n",
    "\n",
    "meta_data.insert(6,'label',label_list)\n",
    "\n",
    "meta_data = meta_data.drop(meta_data[meta_data['label'] == 4].index)\n",
    "meta_data = meta_data.drop(meta_data[meta_data['label'] == 0].index)\n",
    "meta_data['label'] = meta_data['label'] - 1\n",
    "\n",
    "\n",
    "\n",
    "meta_data = meta_data.to_csv('cluster_meta_labels.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0   cluster     y     x          E   size  label\n",
      "0           0  004580_A   9.0  26.0   715232.0  108.0      2\n",
      "1           2  003882_A   3.0   3.0    31156.0    6.0      0\n",
      "2           3  009717_G   7.0  62.0  1423016.0  245.0      2\n",
      "3           4  005590_A  32.0  31.0  1014772.0  169.0      2\n",
      "4           5  008859_G  13.0   9.0   397825.0   63.0      2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "meta_data = pd.read_csv('cluster_meta_labels.csv')\n",
    "print(meta_data.head())\n",
    "\n",
    "def load_data_with_label(folder_path, meta_data):\n",
    "    data = []\n",
    "    for file in meta_data['cluster']:\n",
    "        file = file + '.csv'\n",
    "        file_path = os.path.join(folder_path,file)\n",
    "        cluster = pd.read_csv(file_path,header=None).values\n",
    "        cluster = cluster.flatten()\n",
    "        # cluster = np.append(cluster, meta_data[meta_data['cluster'] == file[:-4]][['y', 'x', 'E', 'size']].values.flatten())\n",
    "        data.append(cluster)\n",
    "    combined_array = np.stack(data,axis=0)\n",
    "    print('shape of combined array: ')\n",
    "    print(combined_array.shape)\n",
    "    return combined_array, meta_data[['y', 'x', 'E', 'size']].values, meta_data['label'].values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "meta_scaler = RobustScaler()\n",
    "meta_data_values = meta_scaler.fit_transform(meta_data[['y', 'x', 'E', 'size']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of combined array: \n",
      "(3112, 4096)\n"
     ]
    }
   ],
   "source": [
    "class NumpyArrayDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data.astype(np.float32)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample  # Return only the sample and dummy label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "folder_path = 'clusters_colour_rotations_rescaled'\n",
    "combined_array, meta_data_values, meta_data_labels = load_data_with_label(folder_path, meta_data)\n",
    "\n",
    "meta_data_labels = meta_data_labels.astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normelize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3112, 4096)\n",
      "(3112, 4101)\n",
      "torch.Size([2489, 4101])\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "combined_array_scaled = scaler.fit_transform(np.log(combined_array))\n",
    "print(combined_array.shape)\n",
    "\n",
    "meta_scaler = QuantileTransformer()\n",
    "meta_data_values = meta_scaler.fit_transform(meta_data_values)\n",
    "\n",
    "combined_array_scaled = np.append(combined_array_scaled, meta_data_values, axis=1)\n",
    "\n",
    "combined_array_scaled = np.append(combined_array_scaled, meta_data_labels[:,None], axis=1)\n",
    "#print(combined_array_scaled.T[-5:])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "np.random.shuffle(combined_array_scaled)\n",
    "\n",
    "#meta_data_values = NumpyArrayDataset(meta_data_values, transform=transform)\n",
    "dataset = NumpyArrayDataset(combined_array_scaled, transform=transform)\n",
    "print(combined_array_scaled.shape)\n",
    "\n",
    "# lazy, non-random split\n",
    "test_size = 0.2\n",
    "split_index = int(len(dataset) * (1 - test_size))\n",
    "train_dataset = dataset[:split_index]\n",
    "test_dataset = dataset[split_index:]\n",
    "#train_meta = meta_data_values[:split_index]\n",
    "#test_meta = meta_data_values[split_index:]\n",
    "#train_labels = meta_data_labels[:split_index]\n",
    "#test_labels = meta_data_labels[split_index:]\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "valid_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "print(train_loader.dataset[0].shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chosse device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def create_torch_model_CNN_HPO(trial):\n",
    "    N_conv_layers = trial.suggest_int('N_conv_layers', 1, 3)\n",
    "    N_dense_layers = trial.suggest_int('N_dense_layers', 1, 3)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "    num_classes = 3\n",
    "    class torch_CNN_HPO(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "\n",
    "            ### Flattening:\n",
    "            self.flatten = nn.Flatten(start_dim=0)\n",
    "\n",
    "            self.peram_to_optimize = []\n",
    "            ### dropout\n",
    "            self.dropout = nn.Dropout(dropout_rate, inplace=True)\n",
    "\n",
    "            self.compile_model()\n",
    "        \n",
    "        def create_CNN_layer(self, size_in, filters_in, layer_number):\n",
    "            filters_out = trial.suggest_int('filters_layer'+str(layer_number), 8, 64)\n",
    "            filter_size = trial.suggest_int('filter_size_layer'+str(layer_number), 3, 7)\n",
    "            stride_cnn = trial.suggest_int('stride_layer'+str(layer_number), 1, 3)\n",
    "            padding = trial.suggest_int('padding_layer'+str(layer_number), 1, 3)\n",
    "            pool_size = 3\n",
    "            pool_stride = 1\n",
    "            pool_padding = 1\n",
    "            cnn_layer = nn.Sequential(\n",
    "                nn.Conv2d(filters_in, filters_out, filter_size, stride=stride_cnn, padding=padding),  # 1x256x256 -> 32x128x128\n",
    "                nn.ReLU(True),\n",
    "                nn.MaxPool2d(pool_size, stride=pool_stride, padding=pool_padding)  # 64x64x64 -> 64x32x32\n",
    "            ).to(device)\n",
    "            size_out = (size_in - filter_size + 2*padding) // stride_cnn + 1\n",
    "            return cnn_layer, filters_out, size_out\n",
    "        \n",
    "        def create_liniar_layer(self, input_size, layer_number):\n",
    "            output_size = trial.suggest_int('dense_layer'+str(layer_number), 8, 128)\n",
    "            liniar_layer = nn.Sequential(\n",
    "                nn.Linear(input_size, output_size),\n",
    "                nn.ReLU(True)\n",
    "            ).to(device)\n",
    "            return liniar_layer, output_size\n",
    "        \n",
    "        def create_output_layer(self, input_size, output_size):\n",
    "            output_layer = nn.Sequential(\n",
    "                nn.Linear(input_size, output_size),\n",
    "                nn.ReLU()\n",
    "            ).to(device)\n",
    "            return output_layer\n",
    "        \n",
    "        def compile_model(self):\n",
    "            #print('Model compiled')\n",
    "            model_layers_cnn = []\n",
    "            model_layers_dense = []\n",
    "            size_in = 64\n",
    "            for i in range(N_conv_layers):\n",
    "                if i == 0:\n",
    "                    cnn_layer, filters_out, size_out = self.create_CNN_layer(size_in, 1, i)\n",
    "                else:\n",
    "                    cnn_layer, filters_out, size_out = self.create_CNN_layer(size_out, filters_out, i)\n",
    "                self.peram_to_optimize.append({'params': cnn_layer.parameters()})\n",
    "                model_layers_cnn.append(cnn_layer)\n",
    "\n",
    "            output_size = size_out ** 2 * filters_out\n",
    "            for i in range(N_dense_layers):\n",
    "                if i == 0:\n",
    "                    liniar_layer, output_size = self.create_liniar_layer(output_size, i)\n",
    "                else:\n",
    "                    liniar_layer, output_size = self.create_liniar_layer(output_size, i)\n",
    "                self.peram_to_optimize.append({'params': liniar_layer.parameters()})\n",
    "                model_layers_dense.append(liniar_layer)\n",
    "            \n",
    "            output_layer = self.create_output_layer(output_size + 4, num_classes)\n",
    "            self.peram_to_optimize.append({'params': output_layer.parameters()})\n",
    "        \n",
    "    \n",
    "            \n",
    "            def model_compiled_v2(x, meta):\n",
    "                for layer in model_layers_cnn:\n",
    "                    x = layer(x)\n",
    "                #print(x.shape)\n",
    "                x = self.flatten(x)\n",
    "                x = self.dropout(x)\n",
    "                for layer in model_layers_dense:\n",
    "                    x = layer(x)\n",
    "                y = torch.cat((x, meta), dim=0)\n",
    "                x = output_layer(y)\n",
    "                return x\n",
    "            self.model_compiled = model_compiled_v2\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            \n",
    "            meta = x[-5:-1]\n",
    "            label = x[-1]\n",
    "            x = x[:-5].unflatten(0, (1, 1, 64, 64))\n",
    "\n",
    "            x = self.model_compiled(x, meta)\n",
    "            #print(label.type(), x.type())\n",
    "            return x, label#.type(torch.cuda.ByteTensor)\n",
    "    return torch_CNN_HPO().to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_torch_model_CNN():\n",
    "    N_conv_layers = 1\n",
    "    N_dense_layers = 1\n",
    "    dropout_rate = 0.4\n",
    "    num_classes = 3\n",
    "    \n",
    "    \n",
    "\n",
    "    class torch_CNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "\n",
    "            ### Flattening:\n",
    "            self.flatten = nn.Flatten(start_dim=0)\n",
    "            self.peram_to_optimize = []\n",
    "\n",
    "            ### dropout\n",
    "            self.dropout = nn.Dropout(dropout_rate, inplace=True)\n",
    "\n",
    "            self.compile_model()\n",
    "\n",
    "        \n",
    "        def create_CNN_layer(self, size_in, filters_in):\n",
    "            filters_out = 8\n",
    "            filter_size = 3\n",
    "            stride_cnn = 1\n",
    "            padding = 1\n",
    "            pool_size = 3\n",
    "            pool_stride = 1\n",
    "            pool_padding = 1\n",
    "            cnn_layer = nn.Sequential(\n",
    "                nn.Conv2d(filters_in, filters_out, filter_size, stride=stride_cnn, padding=padding),  # 1x256x256 -> 32x128x128\n",
    "                nn.ReLU(True),\n",
    "                nn.MaxPool2d(pool_size, stride=pool_stride, padding=pool_padding)  # 64x64x64 -> 64x32x32\n",
    "            ).to(device)\n",
    "            size_out = (size_in - filter_size + 2*padding) // stride_cnn + 1\n",
    "            return cnn_layer, filters_out, size_out\n",
    "        \n",
    "        def create_liniar_layer(self, input_size):\n",
    "            output_size = 32\n",
    "            liniar_layer = nn.Sequential(\n",
    "                nn.Linear(input_size, output_size),\n",
    "                nn.ReLU(True)\n",
    "            ).to(device)\n",
    "            return liniar_layer, output_size\n",
    "        \n",
    "        def create_output_layer(self, input_size, output_size):\n",
    "            output_layer = nn.Sequential(\n",
    "                nn.Linear(input_size, output_size),\n",
    "                nn.ReLU()\n",
    "            ).to(device)\n",
    "            return output_layer\n",
    "        \n",
    "        \n",
    "        \n",
    "        def compile_model(self):\n",
    "            model_layers_cnn = []\n",
    "            model_layers_dense = []\n",
    "            size_in = 32\n",
    "            for i in range(N_conv_layers):\n",
    "                if i == 0:\n",
    "                    cnn_layer, filters_out, size_out = self.create_CNN_layer(size_in, 1)\n",
    "                else:\n",
    "                    cnn_layer, filters_out, size_out = self.create_CNN_layer(size_out, filters_out)\n",
    "\n",
    "                self.peram_to_optimize.append({'params': cnn_layer.parameters()})\n",
    "                model_layers_cnn.append(cnn_layer)\n",
    "\n",
    "            #output_size = size_out ** 2 * filters_out\n",
    "            output_size = filters_out\n",
    "            for i in range(N_dense_layers):\n",
    "                if i == 0:\n",
    "                    liniar_layer, output_size = self.create_liniar_layer(output_size)\n",
    "                else:\n",
    "                    liniar_layer, output_size = self.create_liniar_layer(output_size)\n",
    "                self.peram_to_optimize.append({'params': liniar_layer.parameters()})\n",
    "                model_layers_dense.append(liniar_layer)\n",
    "            \n",
    "            output_layer = self.create_output_layer(output_size + 4, num_classes)\n",
    "            self.peram_to_optimize.append({'params': output_layer.parameters()})\n",
    "            \n",
    "            \n",
    "            def model_compiled_v2(x, meta):\n",
    "                for layer in model_layers_cnn:\n",
    "                    x = layer(x)\n",
    "                   \n",
    "                #print(x.shape)\n",
    "                x = torch.mean(x, dim=3)\n",
    "                #print(x.shape)\n",
    "                x = torch.mean(x, dim=2)\n",
    "                #print(x.shape)\n",
    "                x = self.flatten(x)\n",
    "                x = self.dropout(x)\n",
    "                for layer in model_layers_dense:\n",
    "                    x = layer(x)\n",
    "                    \n",
    "                y = torch.cat((x, meta), dim=0)\n",
    "                x = output_layer(y)\n",
    "                \n",
    "                return x\n",
    "            self.model_compiled = model_compiled_v2\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            \n",
    "            meta = x[-5:-1]\n",
    "            label = x[-1]\n",
    "            x = x[:-5].unflatten(0, (1, 1, 64, 64))\n",
    "\n",
    "            x = self.model_compiled(x, meta)\n",
    "            return x, label#.type(torch.cuda.ByteTensor)\n",
    "    return torch_CNN().to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial, dataset_train, dataset_val, num_epochs=30):\n",
    "    # Build model and optimizer.\n",
    "    model = create_torch_model_CNN_HPO(trial)\n",
    "    learning_rate = trial.suggest_float(\"adam_learning_rate\", 1e-7, 1e-1, log=True)\n",
    "    torch.manual_seed(42)\n",
    "    peram_to_optimize = model.peram_to_optimize\n",
    "    \n",
    "    \n",
    "    optimizer = optim.Adam(peram_to_optimize, lr=learning_rate)\n",
    "    \n",
    "    model.train()\n",
    "    # Fit the model to the data\n",
    "    train_losses, valid_losses = train_model(model, num_epochs, dataset_train, dataset_val, optimizer, loss_function)\n",
    "    #model.fit(x=X_train, y = Y_train, epochs=30, validation_data=(X_test, Y_test), verbose=0)\n",
    "    # Find the accuracy\n",
    "    #cce = CategoricalCrossentropy()\n",
    "    #accuracy = cce(Y_test, model(X_test))\n",
    "    #Return accuracy\n",
    "    return valid_losses[-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(output, target):\n",
    "    try:\n",
    "        target = target.type(torch.cuda.ByteTensor)\n",
    "    except:\n",
    "        target = target.type(torch.ByteTensor)\n",
    "    #plt.plot(target.cpu().detach().numpy())\n",
    "    #plt.show()\n",
    "    #print(output, target)\n",
    "    #loss = F.cross_entropy(output, target)\n",
    "    \n",
    "\n",
    "    #cce = CategoricalCrossentropy()\n",
    "    #loss = cce(y_true, output.detach().numpy())\n",
    "\n",
    "    loss = nn.CrossEntropyLoss(reduction='sum')(output, target)\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_epoch(dataset_train, model, optimizer, loss_function):\n",
    "    running_loss = 0\n",
    "    accuracy = 0\n",
    "    \n",
    "    for batch in dataset_train:\n",
    "        optimizer.zero_grad()\n",
    "        #print(next(model.parameters()).device, batch, model.device)\n",
    "        output, label_train = model(batch)\n",
    "        #print(label_train, output)\n",
    "        loss = loss_function(output, label_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(loss.item())\n",
    "        running_loss += loss.item()\n",
    "        accuracy += 1#cce(label_train, output)\n",
    "    return running_loss / len(dataset_train), accuracy / len(dataset_train)\n",
    "\n",
    "def validate_epoch(dataset_val, model, loss_function):\n",
    "    running_loss = 0\n",
    "    accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataset_val:\n",
    "            output, label_val = model(batch)\n",
    "            loss = loss_function(output, label_val)\n",
    "            running_loss += loss.item()\n",
    "            accuracy += 1#cce(label_val, output)\n",
    "\n",
    "    return running_loss / len(dataset_val), accuracy / len(dataset_val)\n",
    "\n",
    "def train_model(model, num_epochs, dataset_train, dataset_val, optimizer, loss_function):\n",
    "    \n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracys = []\n",
    "    valid_losses = []\n",
    "    validate_accuracys = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        train_loss, train_accuracy = train_epoch(dataset_train, model, optimizer, loss_function)\n",
    "        valid_loss, validate_accuracy = validate_epoch(dataset_val, model, loss_function)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracys.append(train_accuracy)\n",
    "        valid_losses.append(valid_loss)\n",
    "        validate_accuracys.append(validate_accuracy)\n",
    "        end_time = time.time()\n",
    "        print(f\"Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Duration: {end_time - start_time:.2f} sec\")\n",
    "    return train_losses, valid_losses, train_accuracys, validate_accuracys\n",
    "\n",
    "def plot_losses(train_losses, valid_losses, train_accuracys, validate_accuracys):\n",
    "    fig, ax = plt.subplots(nrows=2, sharex=True)\n",
    "    ax[0].plot(train_losses, label='Train Loss')\n",
    "    ax[0].plot(valid_losses, label='Valid Loss')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_yscale('log')\n",
    "    ax[1].plot(train_accuracys, label='Train Accuracy')\n",
    "    ax[1].plot(validate_accuracys, label='Valid Accuracy')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5514, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1., 0, 0])\n",
    "b = 1\n",
    "a = torch.tensor(a)\n",
    "b = torch.tensor(b)\n",
    "print(loss_function(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data to gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = train_loader.dataset[0]\n",
    "dataset_train = dataset_train.to(device)\n",
    "dataset_val = valid_loader.dataset[0]\n",
    "dataset_val = dataset_val.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Train Loss: 0.8016, Valid Loss: 0.6435, Duration: 11.44 sec\n",
      "Epoch: 2/10, Train Loss: 0.5940, Valid Loss: 0.5685, Duration: 11.62 sec\n",
      "Epoch: 3/10, Train Loss: 0.5481, Valid Loss: 0.5463, Duration: 11.37 sec\n",
      "Epoch: 4/10, Train Loss: 0.5335, Valid Loss: 0.5220, Duration: 11.27 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(peram_to_optimize, lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 13\u001b[0m train_losses, valid_losses, train_accuracys, validate_accuracys \u001b[38;5;241m=\u001b[39m train_model(model, num_epochs, dataset_train, dataset_val, optimizer, loss_function)\n",
      "Cell \u001b[0;32mIn[83], line 307\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, num_epochs, dataset_train, dataset_val, optimizer, loss_function)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    306\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 307\u001b[0m     train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m train_epoch(dataset_train, model, optimizer, loss_function)\n\u001b[1;32m    308\u001b[0m     valid_loss, validate_accuracy \u001b[38;5;241m=\u001b[39m validate_epoch(dataset_val, model, loss_function)\n\u001b[1;32m    309\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[83], line 275\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(dataset_train, model, optimizer, loss_function)\u001b[0m\n\u001b[1;32m    273\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m#print(next(model.parameters()).device, batch, model.device)\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m output, label_train \u001b[38;5;241m=\u001b[39m model(batch)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m#print(label_train, output)\u001b[39;00m\n\u001b[1;32m    277\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(output, label_train)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[83], line 214\u001b[0m, in \u001b[0;36mcreate_torch_model_CNN.<locals>.torch_CNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    211\u001b[0m label \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    212\u001b[0m x \u001b[38;5;241m=\u001b[39m x[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m]\u001b[38;5;241m.\u001b[39munflatten(\u001b[38;5;241m0\u001b[39m, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m))\n\u001b[0;32m--> 214\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_compiled(x, meta)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, label\n",
      "Cell \u001b[0;32mIn[83], line 189\u001b[0m, in \u001b[0;36mcreate_torch_model_CNN.<locals>.torch_CNN.compile_model.<locals>.model_compiled_v2\u001b[0;34m(x, meta)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_compiled_v2\u001b[39m(x, meta):\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model_layers_cnn:\n\u001b[0;32m--> 189\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x)\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;66;03m#print(x.shape)\u001b[39;00m\n\u001b[1;32m    192\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/pooling.py:164\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    165\u001b[0m                         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, ceil_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mceil_mode,\n\u001b[1;32m    166\u001b[0m                         return_indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_indices)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_jit_internal.py:499\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:796\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    795\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmax_pool2d(\u001b[38;5;28minput\u001b[39m, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "model = create_torch_model_CNN()\n",
    "\n",
    "learning_rate = 0.001\n",
    "torch.manual_seed(42)\n",
    "\n",
    "#print(peram_to_optimize)\n",
    "peram_to_optimize = model.peram_to_optimize\n",
    "\n",
    "optimizer = optim.Adam(peram_to_optimize, lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "train_losses, valid_losses, train_accuracys, validate_accuracys = train_model(model, num_epochs, dataset_train, dataset_val, optimizer, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses, valid_losses, train_accuracys, validate_accuracys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-10 15:48:31,388] A new study created in memory with name: no-name-1b6908e3-98f5-4628-8582-3b919e9d0664\n",
      "/home/sk0rt3/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n",
      "[W 2024-06-10 15:51:15,421] Trial 0 failed with parameters: {'N_conv_layers': 2, 'N_dense_layers': 1, 'dropout_rate': 0.36962796925706515, 'filters_layer0': 30, 'filter_size_layer0': 5, 'stride_layer0': 1, 'padding_layer0': 3, 'filters_layer1': 49, 'filter_size_layer1': 4, 'stride_layer1': 1, 'padding_layer1': 2, 'dense_layer0': 126, 'adam_learning_rate': 7.754607795582873e-05} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sk0rt3/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3387/1615904111.py\", line 4, in <lambda>\n",
      "    study.optimize(lambda trial: objective(trial, dataset_train, dataset_val, num_epochs), n_trials=30)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3387/4132967002.py\", line 235, in objective\n",
      "    train_losses, valid_losses = train_model(model, num_epochs, dataset_train, dataset_val, optimizer, loss_function)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3387/4132967002.py\", line 307, in train_model\n",
      "    train_loss, train_accuracy = train_epoch(dataset_train, model, optimizer, loss_function)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3387/4132967002.py\", line 279, in train_epoch\n",
      "    optimizer.step()\n",
      "  File \"/home/sk0rt3/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 385, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sk0rt3/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sk0rt3/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py\", line 166, in step\n",
      "    adam(\n",
      "  File \"/home/sk0rt3/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py\", line 316, in adam\n",
      "    func(params,\n",
      "  File \"/home/sk0rt3/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py\", line 441, in _single_tensor_adam\n",
      "    param.addcdiv_(exp_avg, denom, value=-step_size)\n",
      "KeyboardInterrupt\n",
      "[W 2024-06-10 15:51:15,428] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      3\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: objective(trial, dataset_train, dataset_val, num_epochs), n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m create_torch_model_CNN(trial)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#model = model.to(device)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     _optimize(\n\u001b[1;32m    452\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    453\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    454\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[1;32m    455\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    456\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[1;32m    457\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[1;32m    458\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    459\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[1;32m    460\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[1;32m    461\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         _optimize_sequential(\n\u001b[1;32m     63\u001b[0m             study,\n\u001b[1;32m     64\u001b[0m             func,\n\u001b[1;32m     65\u001b[0m             n_trials,\n\u001b[1;32m     66\u001b[0m             timeout,\n\u001b[1;32m     67\u001b[0m             catch,\n\u001b[1;32m     68\u001b[0m             callbacks,\n\u001b[1;32m     69\u001b[0m             gc_after_trial,\n\u001b[1;32m     70\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     71\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     72\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[1;32m     73\u001b[0m         )\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[88], line 4\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      2\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      3\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: objective(trial, dataset_train, dataset_val, num_epochs), n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m create_torch_model_CNN(trial)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#model = model.to(device)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[87], line 235\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial, dataset_train, dataset_val, num_epochs)\u001b[0m\n\u001b[1;32m    233\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Fit the model to the data\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m train_losses, valid_losses \u001b[38;5;241m=\u001b[39m train_model(model, num_epochs, dataset_train, dataset_val, optimizer, loss_function)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m#model.fit(x=X_train, y = Y_train, epochs=30, validation_data=(X_test, Y_test), verbose=0)\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Find the accuracy\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m#cce = CategoricalCrossentropy()\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m#accuracy = cce(Y_test, model(X_test))\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m#Return accuracy\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m valid_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[87], line 307\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, num_epochs, dataset_train, dataset_val, optimizer, loss_function)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    306\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 307\u001b[0m     train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m train_epoch(dataset_train, model, optimizer, loss_function)\n\u001b[1;32m    308\u001b[0m     valid_loss, validate_accuracy \u001b[38;5;241m=\u001b[39m validate_epoch(dataset_val, model, loss_function)\n\u001b[1;32m    309\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[87], line 279\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(dataset_train, model, optimizer, loss_function)\u001b[0m\n\u001b[1;32m    277\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(output, label_train)\n\u001b[1;32m    278\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 279\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m#print(loss.item())\u001b[39;00m\n\u001b[1;32m    281\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    158\u001b[0m         group,\n\u001b[1;32m    159\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    164\u001b[0m         state_steps)\n\u001b[0;32m--> 166\u001b[0m     adam(\n\u001b[1;32m    167\u001b[0m         params_with_grad,\n\u001b[1;32m    168\u001b[0m         grads,\n\u001b[1;32m    169\u001b[0m         exp_avgs,\n\u001b[1;32m    170\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    171\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    172\u001b[0m         state_steps,\n\u001b[1;32m    173\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    174\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    175\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    176\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    177\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    178\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    179\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    180\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    181\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    182\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    183\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    184\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    185\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    186\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    187\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 316\u001b[0m func(params,\n\u001b[1;32m    317\u001b[0m      grads,\n\u001b[1;32m    318\u001b[0m      exp_avgs,\n\u001b[1;32m    319\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    320\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    321\u001b[0m      state_steps,\n\u001b[1;32m    322\u001b[0m      amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    323\u001b[0m      has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    324\u001b[0m      beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    325\u001b[0m      beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    326\u001b[0m      lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    327\u001b[0m      weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    328\u001b[0m      eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    329\u001b[0m      maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    330\u001b[0m      capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    331\u001b[0m      differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    332\u001b[0m      grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    333\u001b[0m      found_inf\u001b[38;5;241m=\u001b[39mfound_inf)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:441\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 441\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "num_epochs = 10\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(lambda trial: objective(trial, dataset_train, dataset_val, num_epochs), n_trials=30)\n",
    "\n",
    "model = create_torch_model_CNN(trial)\n",
    "#model = model.to(device)\n",
    "\n",
    "learning_rate = 0.001\n",
    "torch.manual_seed(42)\n",
    "peram_to_optimize = model.parameters()\n",
    "optimizer = optim.Adam(peram_to_optimize, lr=learning_rate)\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15162\n"
     ]
    }
   ],
   "source": [
    "print(42*19*19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sk0rt3/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def test_func(model, dataset):\n",
    "    output = np.zeros(len(dataset))\n",
    "    label = np.zeros(len(dataset))\n",
    "    for batch in dataset:\n",
    "        model(batch)\n",
    "        \n",
    "for i in range(10):\n",
    "    test_func(model, dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu False\n",
      "torch.cuda.ByteTensor torch.FloatTensor\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 31\u001b[0m\n\u001b[1;32m     24\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice, dataset_train\u001b[38;5;241m.\u001b[39mis_cuda)\n\u001b[0;32m---> 31\u001b[0m train_losses, valid_losses \u001b[38;5;241m=\u001b[39m train_model(model, num_epochs, dataset_train, dataset_val, optimizer, loss_function)\n\u001b[1;32m     33\u001b[0m plot_losses(train_losses, valid_losses)\n",
      "Cell \u001b[0;32mIn[7], line 168\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, num_epochs, dataset_train, dataset_val, optimizer, loss_function)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    167\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 168\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_epoch(dataset_train, model, optimizer, loss_function)\n\u001b[1;32m    169\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m validate_epoch(dataset_val, model, loss_function)\n\u001b[1;32m    170\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[7], line 146\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(dataset_train, model, optimizer, loss_function)\u001b[0m\n\u001b[1;32m    144\u001b[0m output, label_train \u001b[38;5;241m=\u001b[39m model(batch)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mprint\u001b[39m(label_train\u001b[38;5;241m.\u001b[39mtype(), output\u001b[38;5;241m.\u001b[39mtype())\n\u001b[0;32m--> 146\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(output, label_train)\n\u001b[1;32m    147\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    148\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[7], line 131\u001b[0m, in \u001b[0;36mloss_function\u001b[0;34m(output, target)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_function\u001b[39m(output, target):\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(output, target)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3085\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)"
     ]
    }
   ],
   "source": [
    "model = create_torch_model_CNN(3).to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "dataset_train = train_loader.dataset[0]\n",
    "dataset_val = valid_loader.dataset[0]\n",
    "\n",
    "\n",
    "dataset_train = dataset_train.to(device)\n",
    "dataset_val = dataset_val.to(device)\n",
    "\n",
    "learning_rate = 0.001\n",
    "torch.manual_seed(42)\n",
    "peram_to_optimize = model.parameters()\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(peram_to_optimize, lr=learning_rate)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "\n",
    "print(next(model.parameters()).device, dataset_train.is_cuda)\n",
    "\n",
    "\n",
    "train_losses, valid_losses = train_model(model, num_epochs, dataset_train, dataset_val, optimizer, loss_function)\n",
    "\n",
    "plot_losses(train_losses, valid_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
