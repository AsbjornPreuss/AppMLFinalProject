{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-09 11:21:34.428932: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-09 11:21:35.840484: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, QuantileTransformer, MinMaxScaler\n",
    "\n",
    "from umap import UMAP\n",
    "from sklearn.cluster import DBSCAN\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add labels to meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_raw = [np.load('./labels/'+x) for x in os.listdir('./labels/') if x.endswith('.npy')]\n",
    "\n",
    "label_dict = {}\n",
    "value_array = []\n",
    "for values in labels_raw:\n",
    "    for value,ind in zip(values[0],values[1]):\n",
    "        value_array.append(value)\n",
    "        #print(value)\n",
    "        label_dict[value] = ind\n",
    "\n",
    "meta_data = pd.read_csv('cluster_meta.csv')\n",
    "file_index = meta_data['cluster'].values\n",
    "\n",
    "label_list = [label_dict[int(x[:-2])] for x in file_index]\n",
    "\n",
    "meta_data.insert(5,'label',label_list)\n",
    "\n",
    "meta_data.drop(meta_data[meta_data['label'] == 4].index)\n",
    "meta_data.drop(meta_data[meta_data['label'] == 0].index)\n",
    "meta_data['label'] = meta_data['label'] - 1\n",
    "\n",
    "meta_data = meta_data.to_csv('cluster_meta_labels.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0   cluster     y     x          E  label   size\n",
      "0           0  004580_A   9.0  26.0   715232.0      2  108.0\n",
      "1           1  011701_G  15.0  17.0  1184202.0      3  179.0\n",
      "2           2  003882_A   3.0   3.0    31156.0      0    6.0\n",
      "3           3  009717_G   7.0  62.0  1423016.0      2  245.0\n",
      "4           4  005590_A  32.0  31.0  1014772.0      2  169.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "meta_data = pd.read_csv('cluster_meta_labels.csv')\n",
    "print(meta_data.head())\n",
    "\n",
    "def load_data_with_label(folder_path, meta_data):\n",
    "    data = []\n",
    "    for file in meta_data['cluster']:\n",
    "        file = file + '.csv'\n",
    "        file_path = os.path.join(folder_path,file)\n",
    "        cluster = pd.read_csv(file_path,header=None).values\n",
    "        cluster = cluster.flatten()\n",
    "        # cluster = np.append(cluster, meta_data[meta_data['cluster'] == file[:-4]][['y', 'x', 'E', 'size']].values.flatten())\n",
    "        data.append(cluster)\n",
    "    combined_array = np.stack(data,axis=0)\n",
    "    print('shape of combined array: ')\n",
    "    print(combined_array.shape)\n",
    "    return combined_array, meta_data[['y', 'x', 'E', 'size']].values, meta_data['label'].values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "meta_scaler = RobustScaler()\n",
    "meta_data_values = meta_scaler.fit_transform(meta_data[['y', 'x', 'E', 'size']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of combined array: \n",
      "(3936, 4096)\n"
     ]
    }
   ],
   "source": [
    "class NumpyArrayDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data.astype(np.float32)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample  # Return only the sample and dummy label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "folder_path = 'clusters_colour_rotations_rescaled'\n",
    "combined_array, meta_data_values, meta_data_labels = load_data_with_label(folder_path, meta_data)\n",
    "\n",
    "meta_data_labels = meta_data_labels.astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normelize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3936, 4096)\n",
      "(3936, 4101)\n",
      "torch.Size([3148, 4101])\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "combined_array_scaled = scaler.fit_transform(np.log(combined_array))\n",
    "print(combined_array.shape)\n",
    "\n",
    "meta_scaler = QuantileTransformer()\n",
    "meta_data_values = meta_scaler.fit_transform(meta_data_values)\n",
    "\n",
    "combined_array_scaled = np.append(combined_array_scaled, meta_data_values, axis=1)\n",
    "\n",
    "combined_array_scaled = np.append(combined_array_scaled, meta_data_labels[:,None], axis=1)\n",
    "#print(combined_array_scaled.T[-5:])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "np.random.shuffle(combined_array_scaled)\n",
    "\n",
    "#meta_data_values = NumpyArrayDataset(meta_data_values, transform=transform)\n",
    "dataset = NumpyArrayDataset(combined_array_scaled, transform=transform)\n",
    "print(combined_array_scaled.shape)\n",
    "\n",
    "# lazy, non-random split\n",
    "test_size = 0.2\n",
    "split_index = int(len(dataset) * (1 - test_size))\n",
    "train_dataset = dataset[:split_index]\n",
    "test_dataset = dataset[split_index:]\n",
    "#train_meta = meta_data_values[:split_index]\n",
    "#test_meta = meta_data_values[split_index:]\n",
    "#train_labels = meta_data_labels[:split_index]\n",
    "#test_labels = meta_data_labels[split_index:]\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "valid_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "print(train_loader.dataset[0].shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chosse device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def create_torch_model_CNN(trial):\n",
    "    N_conv_layers = 2 #trial.suggest_int('N_conv_layers', 1, 3)\n",
    "    N_dense_layers = 2 #trial.suggest_int('N_dense_layers', 1, 3)\n",
    "    dropout_rate = 0.5 #trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "    num_classes = 3\n",
    "    class torch_CNN(nn.Module):\n",
    "        def __init__(self, output_size):\n",
    "            super().__init__()\n",
    "\n",
    "            ### Convolutional section:\n",
    "            #self.cnn_layer = nn.Sequential(\n",
    "            #    nn.Conv2d(1, 32, 3, stride=1, padding=1),  # 1x64x64 -> 32x64x64\n",
    "            #    nn.ReLU(True),\n",
    "            #    nn.Conv2d(32, 64, 3, stride=1, padding=1),  # 32x128x128 -> 64x64x64\n",
    "            #    nn.ReLU(True),\n",
    "            #    nn.MaxPool2d(2)  # 64x64x64 -> 64x32x32\n",
    "            #)\n",
    "\n",
    "            ### Flattening:\n",
    "            self.flatten = nn.Flatten(start_dim=0)\n",
    "\n",
    "            ### Linerar section\n",
    "            \n",
    "            #self.linerar_layer = nn.Sequential(\n",
    "            #    nn.Linear(65536, 128),\n",
    "            #    nn.ReLU(True)\n",
    "            #)\n",
    "            \n",
    "            ### output layer\n",
    "            #self.output_layer = nn.Sequential(\n",
    "            #    nn.Linear(128, output_size),\n",
    "            #    nn.Softmax()\n",
    "            #)\n",
    "\n",
    "            ### dropout\n",
    "            self.dropout = nn.Dropout(dropout_rate, inplace=True)\n",
    "\n",
    "            self.compile_model()\n",
    "        \n",
    "        def create_CNN_layer(self, filters_in, filters_out):\n",
    "            #filters_out = 32\n",
    "            filter_size = 3\n",
    "            stride_cnn = 1\n",
    "            padding = 1\n",
    "            pool_size = 3\n",
    "            pool_stride = 1\n",
    "            pool_padding = 1\n",
    "            cnn_layer = nn.Sequential(\n",
    "                nn.Conv2d(filters_in, filters_out, filter_size, stride=stride_cnn, padding=padding),  # 1x256x256 -> 32x128x128\n",
    "                nn.ReLU(True),\n",
    "                nn.MaxPool2d(pool_size, stride=pool_stride, padding=pool_padding)  # 64x64x64 -> 64x32x32\n",
    "            ).to(device)\n",
    "            return cnn_layer, filters_out\n",
    "        \n",
    "        def create_liniar_layer(self, input_size, output_size):\n",
    "            liniar_layer = nn.Sequential(\n",
    "                nn.Linear(input_size, output_size),\n",
    "                nn.ReLU(True)\n",
    "            ).to(device)\n",
    "            return liniar_layer, output_size\n",
    "        \n",
    "        def create_output_layer(self, input_size, output_size):\n",
    "            output_layer = nn.Sequential(\n",
    "                nn.Linear(input_size, output_size),\n",
    "                nn.Softmax()\n",
    "            ).to(device)\n",
    "            return output_layer\n",
    "        \n",
    "        def compile_model(self):\n",
    "            #print('Model compiled')\n",
    "            cnn_layer0, filters_out0 = self.create_CNN_layer(1, 32)\n",
    "            cnn_layer1, filters_out1 = self.create_CNN_layer(filters_out0, 64)\n",
    "            liniar_in = 64 * 64 * filters_out1\n",
    "            liniar_layer0, output_size0 = self.create_liniar_layer(liniar_in, 128)\n",
    "            liniar_layer1, output_size1 = self.create_liniar_layer(output_size0, 64)\n",
    "            output_layer = self.create_output_layer(output_size1 + 4, num_classes)\n",
    "\n",
    "            def model_compiled(x, meta):\n",
    "                x = cnn_layer0(x)\n",
    "                x = cnn_layer1(x)\n",
    "                x = self.flatten(x)\n",
    "                x = self.dropout(x)\n",
    "                x = liniar_layer0(x)\n",
    "                x = liniar_layer1(x)\n",
    "                y = torch.cat((x, meta), dim=0)\n",
    "                x = output_layer(y)\n",
    "                return x\n",
    "            self.model_compiled = model_compiled\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            \n",
    "            meta = x[-5:-1]\n",
    "            label = x[-1]\n",
    "            x = x[:-5].unflatten(0, (1, 1, 64, 64))\n",
    "\n",
    "            x = self.model_compiled(x, meta)\n",
    "           \n",
    "            return x, label.type(torch.cuda.ByteTensor)\n",
    "    return torch_CNN(num_classes).to(device)\n",
    "\n",
    "\n",
    "def create_tf_optimizer(trial):\n",
    "    # We optimize the choice of optimizers as well as their parameters.\n",
    "    kwargs = {}\n",
    "    \n",
    "    optimizer_selected = \"Adam\"\n",
    "    \n",
    "    kwargs[\"learning_rate\"] = trial.suggest_float(\"adam_learning_rate\", 1e-7, 1e-1, log=True)\n",
    "\n",
    "    optimizer = getattr(tf.optimizers, optimizer_selected)(**kwargs)\n",
    "    return optimizer\n",
    "\n",
    "def objective(trial, X_train, Y_train, X_test, Y_test):\n",
    "    # Build model and optimizer.\n",
    "    model = create_tf_model_CNN(trial)\n",
    "    optimizer = create_tf_optimizer(trial)\n",
    "    model.compile(optimizer, loss=CategoricalCrossentropy())\n",
    "    # Fit the model to the data\n",
    "    model.fit(x=X_train, y = Y_train, epochs=30, validation_data=(X_test, Y_test), verbose=0)\n",
    "    # Find the accuracy\n",
    "    cce = CategoricalCrossentropy()\n",
    "    accuracy = cce(Y_test, model(X_test))\n",
    "    # Return accuracy\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(output, target):\n",
    "    return F.cross_entropy(output, target)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_epoch(dataset_train, model, optimizer, loss_function):\n",
    "    running_loss = 0\n",
    "\n",
    "    \n",
    "    for batch in dataset_train:\n",
    "        optimizer.zero_grad()\n",
    "        #print(next(model.parameters()).device, batch, model.device)\n",
    "        output, label_train = model(batch)\n",
    "        print(label_train.type(), output.type())\n",
    "        loss = loss_function(output, label_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(dataset_train)\n",
    "\n",
    "def validate_epoch(dataset_val, model, loss_function):\n",
    "    running_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataset_val:\n",
    "            output, label_val = model(batch)\n",
    "            loss = loss_function(output, label_val)\n",
    "            running_loss += loss.item()\n",
    "    return running_loss / len(dataset_val)\n",
    "\n",
    "def train_model(model, num_epochs, dataset_train, dataset_val, optimizer, loss_function):\n",
    "    \n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        train_loss = train_epoch(dataset_train, model, optimizer, loss_function)\n",
    "        valid_loss = validate_epoch(dataset_val, model, loss_function)\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        end_time = time.time()\n",
    "        print(f\"Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Duration: {end_time - start_time:.2f} sec\")\n",
    "    return train_losses, valid_losses\n",
    "\n",
    "def plot_losses(train_losses, valid_losses):\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(valid_losses, label='Valid Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data to gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = train_loader.dataset[0]\n",
    "dataset_train = dataset_train.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch_CNN(\n",
       "  (cnn_layer): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=0, end_dim=-1)\n",
       "  (linerar_layer): Sequential(\n",
       "    (0): Linear(in_features=65536, out_features=128, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (output_layer): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=3, bias=True)\n",
       "    (1): Softmax(dim=None)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_torch_model_CNN(3)\n",
    "#model = model.to(device)\n",
    "\n",
    "learning_rate = 0.001\n",
    "torch.manual_seed(42)\n",
    "peram_to_optimize = model.parameters()\n",
    "optimizer = optim.Adam(peram_to_optimize, lr=learning_rate)\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sk0rt3/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def test_func(model, dataset):\n",
    "    output = np.zeros(len(dataset))\n",
    "    label = np.zeros(len(dataset))\n",
    "    for batch in dataset:\n",
    "        model(batch)\n",
    "        \n",
    "for i in range(10):\n",
    "    test_func(model, dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu False\n",
      "torch.cuda.ByteTensor torch.FloatTensor\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 31\u001b[0m\n\u001b[1;32m     24\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice, dataset_train\u001b[38;5;241m.\u001b[39mis_cuda)\n\u001b[0;32m---> 31\u001b[0m train_losses, valid_losses \u001b[38;5;241m=\u001b[39m train_model(model, num_epochs, dataset_train, dataset_val, optimizer, loss_function)\n\u001b[1;32m     33\u001b[0m plot_losses(train_losses, valid_losses)\n",
      "Cell \u001b[0;32mIn[7], line 168\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, num_epochs, dataset_train, dataset_val, optimizer, loss_function)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    167\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 168\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_epoch(dataset_train, model, optimizer, loss_function)\n\u001b[1;32m    169\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m validate_epoch(dataset_val, model, loss_function)\n\u001b[1;32m    170\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[7], line 146\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(dataset_train, model, optimizer, loss_function)\u001b[0m\n\u001b[1;32m    144\u001b[0m output, label_train \u001b[38;5;241m=\u001b[39m model(batch)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mprint\u001b[39m(label_train\u001b[38;5;241m.\u001b[39mtype(), output\u001b[38;5;241m.\u001b[39mtype())\n\u001b[0;32m--> 146\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(output, label_train)\n\u001b[1;32m    147\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    148\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[7], line 131\u001b[0m, in \u001b[0;36mloss_function\u001b[0;34m(output, target)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_function\u001b[39m(output, target):\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(output, target)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3085\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)"
     ]
    }
   ],
   "source": [
    "model = create_torch_model_CNN(3).to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "dataset_train = train_loader.dataset[0]\n",
    "dataset_val = valid_loader.dataset[0]\n",
    "\n",
    "\n",
    "dataset_train = dataset_train.to(device)\n",
    "dataset_val = dataset_val.to(device)\n",
    "\n",
    "learning_rate = 0.001\n",
    "torch.manual_seed(42)\n",
    "peram_to_optimize = model.parameters()\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(peram_to_optimize, lr=learning_rate)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "\n",
    "print(next(model.parameters()).device, dataset_train.is_cuda)\n",
    "\n",
    "\n",
    "train_losses, valid_losses = train_model(model, num_epochs, dataset_train, dataset_val, optimizer, loss_function)\n",
    "\n",
    "plot_losses(train_losses, valid_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
