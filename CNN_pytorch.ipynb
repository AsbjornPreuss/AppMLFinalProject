{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, QuantileTransformer, MinMaxScaler\n",
    "\n",
    "from umap import UMAP\n",
    "from sklearn.cluster import DBSCAN\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add labels to meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_raw = [np.load('./labels/'+x) for x in os.listdir('./labels/') if x.endswith('.npy')]\n",
    "\n",
    "label_dict = {}\n",
    "value_array = []\n",
    "for values in labels_raw:\n",
    "    for value,ind in zip(values[0],values[1]):\n",
    "        value_array.append(value)\n",
    "        #print(value)\n",
    "        label_dict[value] = ind\n",
    "\n",
    "meta_data = pd.read_csv('cluster_meta.csv')\n",
    "file_index = meta_data['cluster'].values\n",
    "\n",
    "label_list = [label_dict[int(x[:-2])] for x in file_index]\n",
    "\n",
    "meta_data.insert(6,'label',label_list)\n",
    "\n",
    "meta_data = meta_data.drop(meta_data[meta_data['label'] == 4].index)\n",
    "meta_data = meta_data.drop(meta_data[meta_data['label'] == 0].index)\n",
    "meta_data['label'] = meta_data['label'] - 1\n",
    "\n",
    "\n",
    "\n",
    "meta_data = meta_data.to_csv('cluster_meta_labels.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0   cluster     y     x          E   size  label\n",
      "0           0  004580_A   9.0  26.0   715232.0  108.0      2\n",
      "1           2  003882_A   3.0   3.0    31156.0    6.0      0\n",
      "2           3  009717_G   7.0  62.0  1423016.0  245.0      2\n",
      "3           4  005590_A  32.0  31.0  1014772.0  169.0      2\n",
      "4           5  008859_G  13.0   9.0   397825.0   63.0      2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "meta_data = pd.read_csv('cluster_meta_labels.csv')\n",
    "print(meta_data.head())\n",
    "\n",
    "def load_data_with_label(folder_path, meta_data):\n",
    "    data = []\n",
    "    for file in meta_data['cluster']:\n",
    "        file = file + '.csv'\n",
    "        file_path = os.path.join(folder_path,file)\n",
    "        cluster = pd.read_csv(file_path,header=None).values\n",
    "        cluster = cluster.flatten()\n",
    "        # cluster = np.append(cluster, meta_data[meta_data['cluster'] == file[:-4]][['y', 'x', 'E', 'size']].values.flatten())\n",
    "        data.append(cluster)\n",
    "    combined_array = np.stack(data,axis=0)\n",
    "    print('shape of combined array: ')\n",
    "    print(combined_array.shape)\n",
    "    return combined_array, meta_data[['y', 'x', 'E', 'size']].values, meta_data['label'].values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "meta_scaler = RobustScaler()\n",
    "meta_data_values = meta_scaler.fit_transform(meta_data[['y', 'x', 'E', 'size']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of combined array: \n",
      "(3112, 4096)\n"
     ]
    }
   ],
   "source": [
    "class NumpyArrayDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data.astype(np.float32)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample  # Return only the sample and dummy label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "folder_path = 'clusters_colour_rotations_rescaled'\n",
    "combined_array, meta_data_values, meta_data_labels = load_data_with_label(folder_path, meta_data)\n",
    "\n",
    "meta_data_labels = meta_data_labels.astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normelize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3112, 4096)\n",
      "(3112, 4101)\n",
      "torch.Size([2489, 4101])\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "combined_array_scaled = scaler.fit_transform(np.log(combined_array))\n",
    "print(combined_array.shape)\n",
    "\n",
    "meta_scaler = QuantileTransformer()\n",
    "meta_data_values = meta_scaler.fit_transform(meta_data_values)\n",
    "\n",
    "combined_array_scaled = np.append(combined_array_scaled, meta_data_values, axis=1)\n",
    "\n",
    "combined_array_scaled = np.append(combined_array_scaled, meta_data_labels[:,None], axis=1)\n",
    "#print(combined_array_scaled.T[-5:])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "np.random.shuffle(combined_array_scaled)\n",
    "\n",
    "#meta_data_values = NumpyArrayDataset(meta_data_values, transform=transform)\n",
    "dataset = NumpyArrayDataset(combined_array_scaled, transform=transform)\n",
    "print(combined_array_scaled.shape)\n",
    "\n",
    "# lazy, non-random split\n",
    "test_size = 0.2\n",
    "split_index = int(len(dataset) * (1 - test_size))\n",
    "train_dataset = dataset[:split_index]\n",
    "test_dataset = dataset[split_index:]\n",
    "#train_meta = meta_data_values[:split_index]\n",
    "#test_meta = meta_data_values[split_index:]\n",
    "#train_labels = meta_data_labels[:split_index]\n",
    "#test_labels = meta_data_labels[split_index:]\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "valid_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "print(train_loader.dataset[0].shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chosse device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def create_torch_model_CNN_HPO(trial):\n",
    "    N_conv_layers = trial.suggest_int('N_conv_layers', 1, 3)\n",
    "    N_dense_layers = trial.suggest_int('N_dense_layers', 1, 3)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "    num_classes = 3\n",
    "    class torch_CNN_HPO(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "\n",
    "            ### Flattening:\n",
    "            self.flatten = nn.Flatten(start_dim=0)\n",
    "\n",
    "\n",
    "            ### dropout\n",
    "            self.dropout = nn.Dropout(dropout_rate, inplace=True)\n",
    "\n",
    "            self.compile_model()\n",
    "        \n",
    "        def create_CNN_layer(self, size_in, filters_in, layer_number):\n",
    "            filters_out = trial.suggest_int('filters_layer'+str(layer_number), 8, 64)\n",
    "            filter_size = trial.suggest_int('filter_size_layer'+str(layer_number), 3, 7)\n",
    "            stride_cnn = trial.suggest_int('stride_layer'+str(layer_number), 1, 3)\n",
    "            padding = trial.suggest_int('padding_layer'+str(layer_number), 1, 3)\n",
    "            pool_size = 3\n",
    "            pool_stride = 1\n",
    "            pool_padding = 1\n",
    "            cnn_layer = nn.Sequential(\n",
    "                nn.Conv2d(filters_in, filters_out, filter_size, stride=stride_cnn, padding=padding),  # 1x256x256 -> 32x128x128\n",
    "                nn.ReLU(True),\n",
    "                nn.MaxPool2d(pool_size, stride=pool_stride, padding=pool_padding)  # 64x64x64 -> 64x32x32\n",
    "            ).to(device)\n",
    "            size_out = (size_in - filter_size + 2*padding) // stride_cnn + 1\n",
    "            return cnn_layer, filters_out, size_out\n",
    "        \n",
    "        def create_liniar_layer(self, input_size, layer_number):\n",
    "            output_size = trial.suggest_int('dense_layer'+str(layer_number), 8, 128)\n",
    "            liniar_layer = nn.Sequential(\n",
    "                nn.Linear(input_size, output_size),\n",
    "                nn.ReLU(True)\n",
    "            ).to(device)\n",
    "            return liniar_layer, output_size\n",
    "        \n",
    "        def create_output_layer(self, input_size, output_size):\n",
    "            output_layer = nn.Sequential(\n",
    "                nn.Linear(input_size, output_size),\n",
    "                nn.Softmax()\n",
    "            ).to(device)\n",
    "            return output_layer\n",
    "        \n",
    "        def compile_model(self):\n",
    "            #print('Model compiled')\n",
    "            model_layers_cnn = []\n",
    "            model_layers_dense = []\n",
    "            size_in = 64\n",
    "            for i in range(N_conv_layers):\n",
    "                if i == 0:\n",
    "                    cnn_layer, filters_out, size_out = self.create_CNN_layer(size_in, 1, i)\n",
    "                else:\n",
    "                    cnn_layer, filters_out, size_out = self.create_CNN_layer(size_out, filters_out, i)\n",
    "            \n",
    "                model_layers_cnn.append(cnn_layer)\n",
    "\n",
    "            output_size = size_out ** 2 * filters_out\n",
    "            for i in range(N_dense_layers):\n",
    "                if i == 0:\n",
    "                    liniar_layer, output_size = self.create_liniar_layer(output_size, i)\n",
    "                else:\n",
    "                    liniar_layer, output_size = self.create_liniar_layer(output_size, i)\n",
    "                model_layers_dense.append(liniar_layer)\n",
    "            \n",
    "            output_layer = self.create_output_layer(output_size + 4, num_classes)\n",
    "        \n",
    "\n",
    "            \n",
    "            def model_compiled_v2(x, meta):\n",
    "                for layer in model_layers_cnn:\n",
    "                    x = layer(x)\n",
    "                #print(x.shape)\n",
    "                x = self.flatten(x)\n",
    "                x = self.dropout(x)\n",
    "                for layer in model_layers_dense:\n",
    "                    x = layer(x)\n",
    "                y = torch.cat((x, meta), dim=0)\n",
    "                x = output_layer(y)\n",
    "                return x\n",
    "            self.model_compiled = model_compiled_v2\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            \n",
    "            meta = x[-5:-1]\n",
    "            label = x[-1]\n",
    "            x = x[:-5].unflatten(0, (1, 1, 64, 64))\n",
    "\n",
    "            x = self.model_compiled(x, meta)\n",
    "            #print(label.type(), x.type())\n",
    "            return x, label#.type(torch.cuda.ByteTensor)\n",
    "    return torch_CNN_HPO().to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_torch_model_CNN():\n",
    "    N_conv_layers = 1\n",
    "    N_dense_layers = 1\n",
    "    dropout_rate = 0.4\n",
    "    num_classes = 3\n",
    "    class torch_CNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "\n",
    "            ### Flattening:\n",
    "            self.flatten = nn.Flatten(start_dim=0)\n",
    "\n",
    "\n",
    "            ### dropout\n",
    "            self.dropout = nn.Dropout(dropout_rate, inplace=True)\n",
    "\n",
    "            self.compile_model()\n",
    "        \n",
    "        def create_CNN_layer(self, size_in, filters_in):\n",
    "            filters_out = 8\n",
    "            filter_size = 3\n",
    "            stride_cnn = 1\n",
    "            padding = 1\n",
    "            pool_size = 3\n",
    "            pool_stride = 1\n",
    "            pool_padding = 1\n",
    "            cnn_layer = nn.Sequential(\n",
    "                nn.Conv2d(filters_in, filters_out, filter_size, stride=stride_cnn, padding=padding),  # 1x256x256 -> 32x128x128\n",
    "                nn.ReLU(True),\n",
    "                nn.MaxPool2d(pool_size, stride=pool_stride, padding=pool_padding)  # 64x64x64 -> 64x32x32\n",
    "            ).to(device)\n",
    "            size_out = (size_in - filter_size + 2*padding) // stride_cnn + 1\n",
    "            return cnn_layer, filters_out, size_out\n",
    "        \n",
    "        def create_liniar_layer(self, input_size):\n",
    "            output_size = 64\n",
    "            liniar_layer = nn.Sequential(\n",
    "                nn.Linear(input_size, output_size),\n",
    "                nn.ReLU(True)\n",
    "            ).to(device)\n",
    "            return liniar_layer, output_size\n",
    "        \n",
    "        def create_output_layer(self, input_size, output_size):\n",
    "            output_layer = nn.Sequential(\n",
    "                nn.Linear(input_size, output_size),\n",
    "                nn.Softmax()\n",
    "            ).to(device)\n",
    "            return output_layer\n",
    "        \n",
    "        def compile_model(self):\n",
    "            model_layers_cnn = []\n",
    "            model_layers_dense = []\n",
    "            size_in = 64\n",
    "            for i in range(N_conv_layers):\n",
    "                if i == 0:\n",
    "                    cnn_layer, filters_out, size_out = self.create_CNN_layer(size_in, 1)\n",
    "                else:\n",
    "                    cnn_layer, filters_out, size_out = self.create_CNN_layer(size_out, filters_out)\n",
    "            \n",
    "                model_layers_cnn.append(cnn_layer)\n",
    "\n",
    "            output_size = size_out ** 2 * filters_out\n",
    "            for i in range(N_dense_layers):\n",
    "                if i == 0:\n",
    "                    liniar_layer, output_size = self.create_liniar_layer(output_size)\n",
    "                else:\n",
    "                    liniar_layer, output_size = self.create_liniar_layer(output_size)\n",
    "                model_layers_dense.append(liniar_layer)\n",
    "            \n",
    "            output_layer = self.create_output_layer(output_size + 4, num_classes)\n",
    "            \n",
    "            \n",
    "            def model_compiled_v2(x, meta):\n",
    "                for layer in model_layers_cnn:\n",
    "                    x = layer(x)\n",
    "                #print(x.shape)\n",
    "                x = self.flatten(x)\n",
    "                x = self.dropout(x)\n",
    "                for layer in model_layers_dense:\n",
    "                    x = layer(x)\n",
    "                y = torch.cat((x, meta), dim=0)\n",
    "                x = output_layer(y)\n",
    "                return x\n",
    "            self.model_compiled = model_compiled_v2\n",
    "\n",
    "\n",
    "        def forward(self, x):\n",
    "            \n",
    "            meta = x[-5:-1]\n",
    "            label = x[-1]\n",
    "            x = x[:-5].unflatten(0, (1, 1, 64, 64))\n",
    "\n",
    "            x = self.model_compiled(x, meta)\n",
    "            return x, label#.type(torch.cuda.ByteTensor)\n",
    "    return torch_CNN().to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_torch_optimizer(trial):\n",
    "    # We optimize the choice of optimizers as well as their parameters.\n",
    "    kwargs = {}\n",
    "    \n",
    "    optimizer_selected = \"Adam\"\n",
    "    \n",
    "    kwargs[\"learning_rate\"] = trial.suggest_float(\"adam_learning_rate\", 1e-7, 1e-1, log=True)\n",
    "\n",
    "    optimizer = getattr(tf.optimizers, optimizer_selected)(**kwargs)\n",
    "    return optimizer\n",
    "\n",
    "def objective(trial, dataset_train, dataset_val, num_epochs=30):\n",
    "    # Build model and optimizer.\n",
    "    model = create_torch_model_CNN_HPO(trial)\n",
    "    learning_rate = trial.suggest_float(\"adam_learning_rate\", 1e-7, 1e-1, log=True)\n",
    "    torch.manual_seed(42)\n",
    "    peram_to_optimize = [\n",
    "        {'params': model.parameters()},\n",
    "    ]\n",
    "    \n",
    "    print('perameter list', peram_to_optimize)\n",
    "    optimizer = optim.Adam(peram_to_optimize, lr=learning_rate)\n",
    "    \n",
    "    model.train()\n",
    "    # Fit the model to the data\n",
    "    train_losses, valid_losses = train_model(model, num_epochs, dataset_train, dataset_val, optimizer, loss_function)\n",
    "    #model.fit(x=X_train, y = Y_train, epochs=30, validation_data=(X_test, Y_test), verbose=0)\n",
    "    # Find the accuracy\n",
    "    #cce = CategoricalCrossentropy()\n",
    "    #accuracy = cce(Y_test, model(X_test))\n",
    "    # Return accuracy\n",
    "    return valid_losses[-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loss_function(output, target):\n",
    "    try:\n",
    "        target = target.type(torch.cuda.ByteTensor)\n",
    "    except:\n",
    "        target = target.type(torch.ByteTensor)\n",
    "    #plt.plot(target.cpu().detach().numpy())\n",
    "    #plt.show()\n",
    "    #print(target)\n",
    "    loss = F.cross_entropy(output, target)\n",
    "    #loss = nn.BCELoss(reduction='sum')(output, target)\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_epoch(dataset_train, model, optimizer, loss_function):\n",
    "    running_loss = 0\n",
    "\n",
    "    \n",
    "    for batch in dataset_train:\n",
    "        optimizer.zero_grad()\n",
    "        #print(next(model.parameters()).device, batch, model.device)\n",
    "        output, label_train = model(batch)\n",
    "        #print(label_train.type(), output.type())\n",
    "        loss = loss_function(output, label_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(dataset_train)\n",
    "\n",
    "def validate_epoch(dataset_val, model, loss_function):\n",
    "    running_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataset_val:\n",
    "            output, label_val = model(batch)\n",
    "            loss = loss_function(output, label_val)\n",
    "            running_loss += loss.item()\n",
    "    return running_loss / len(dataset_val)\n",
    "\n",
    "def train_model(model, num_epochs, dataset_train, dataset_val, optimizer, loss_function):\n",
    "    \n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        train_loss = train_epoch(dataset_train, model, optimizer, loss_function)\n",
    "        valid_loss = validate_epoch(dataset_val, model, loss_function)\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        end_time = time.time()\n",
    "        print(f\"Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}, Duration: {end_time - start_time:.2f} sec\")\n",
    "    return train_losses, valid_losses\n",
    "\n",
    "def plot_losses(train_losses, valid_losses):\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(valid_losses, label='Valid Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data to gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = train_loader.dataset[0]\n",
    "dataset_train = dataset_train.to(device)\n",
    "dataset_val = valid_loader.dataset[0]\n",
    "dataset_val = dataset_val.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perameter list [{'params': <generator object Module.parameters at 0x71a0e881e0a0>}]\n",
      "Epoch: 1/10, Train Loss: 1.1171, Valid Loss: 1.1169, Duration: 18.17 sec\n",
      "Epoch: 2/10, Train Loss: 1.1172, Valid Loss: 1.1176, Duration: 17.34 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(peram_to_optimize, lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 14\u001b[0m train_losses, valid_losses \u001b[38;5;241m=\u001b[39m train_model(model, num_epochs, dataset_train, dataset_val, optimizer, loss_function)\n",
      "Cell \u001b[0;32mIn[117], line 288\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, num_epochs, dataset_train, dataset_val, optimizer, loss_function)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    287\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 288\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_epoch(dataset_train, model, optimizer, loss_function)\n\u001b[1;32m    289\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m validate_epoch(dataset_val, model, loss_function)\n\u001b[1;32m    290\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[117], line 267\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(dataset_train, model, optimizer, loss_function)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m#print(label_train.type(), output.type())\u001b[39;00m\n\u001b[1;32m    266\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(output, label_train)\n\u001b[0;32m--> 267\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    268\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    269\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "model = create_torch_model_CNN()\n",
    "\n",
    "learning_rate = 0.001\n",
    "torch.manual_seed(42)\n",
    "peram_to_optimize = [\n",
    "    {'params': model.parameters()},\n",
    "]\n",
    "    \n",
    "\n",
    "optimizer = optim.Adam(peram_to_optimize, lr=learning_rate)\n",
    "model.train()\n",
    "\n",
    "train_losses, valid_losses = train_model(model, num_epochs, dataset_train, dataset_val, optimizer, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-10 12:38:31,983] A new study created in memory with name: no-name-4b31bdcf-2183-4f0a-b602-c036f9acf9db\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perameter list [{'params': <generator object Module.parameters at 0x71a105c32880>}]\n",
      "Epoch: 1/10, Train Loss: 1.1331, Valid Loss: 1.1339, Duration: 26.34 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-06-10 12:39:18,185] Trial 0 failed with parameters: {'filters_layer0': 8, 'filter_size_layer0': 3, 'stride_layer0': 1, 'padding_layer0': 2, 'filters_layer1': 30, 'filter_size_layer1': 7, 'stride_layer1': 2, 'padding_layer1': 1, 'dense_layer0': 93, 'dense_layer1': 8, 'adam_learning_rate': 0.005557915547300454} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_20310/1577682002.py\", line 180, in loss_function\n",
      "    target = target.type(torch.cuda.ByteTensor)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sk0rt3/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_20310/1615904111.py\", line 4, in <lambda>\n",
      "    study.optimize(lambda trial: objective(trial, dataset_train, dataset_val, num_epochs), n_trials=30)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_20310/1577682002.py\", line 166, in objective\n",
      "    train_losses, valid_losses = train_model(model, num_epochs, dataset_train, dataset_val, optimizer, loss_function)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_20310/1577682002.py\", line 225, in train_model\n",
      "    train_loss = train_epoch(dataset_train, model, optimizer, loss_function)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_20310/1577682002.py\", line 203, in train_epoch\n",
      "    loss = loss_function(output, label_train)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_20310/1577682002.py\", line 182, in loss_function\n",
      "    target = target.type(torch.ByteTensor)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2024-06-10 12:39:18,186] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 180\u001b[0m, in \u001b[0;36mloss_function\u001b[0;34m(output, target)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 180\u001b[0m     target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mByteTensor)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      3\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: objective(trial, dataset_train, dataset_val, num_epochs), n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m create_torch_model_CNN(trial)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#model = model.to(device)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     _optimize(\n\u001b[1;32m    452\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    453\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    454\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[1;32m    455\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    456\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[1;32m    457\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[1;32m    458\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    459\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[1;32m    460\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[1;32m    461\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         _optimize_sequential(\n\u001b[1;32m     63\u001b[0m             study,\n\u001b[1;32m     64\u001b[0m             func,\n\u001b[1;32m     65\u001b[0m             n_trials,\n\u001b[1;32m     66\u001b[0m             timeout,\n\u001b[1;32m     67\u001b[0m             catch,\n\u001b[1;32m     68\u001b[0m             callbacks,\n\u001b[1;32m     69\u001b[0m             gc_after_trial,\n\u001b[1;32m     70\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     71\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     72\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[1;32m     73\u001b[0m         )\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[97], line 4\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      2\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      3\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: objective(trial, dataset_train, dataset_val, num_epochs), n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m create_torch_model_CNN(trial)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#model = model.to(device)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[96], line 166\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial, dataset_train, dataset_val, num_epochs)\u001b[0m\n\u001b[1;32m    164\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Fit the model to the data\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m train_losses, valid_losses \u001b[38;5;241m=\u001b[39m train_model(model, num_epochs, dataset_train, dataset_val, optimizer, loss_function)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m#model.fit(x=X_train, y = Y_train, epochs=30, validation_data=(X_test, Y_test), verbose=0)\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Find the accuracy\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m#cce = CategoricalCrossentropy()\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m#accuracy = cce(Y_test, model(X_test))\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# Return accuracy\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m valid_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[96], line 225\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, num_epochs, dataset_train, dataset_val, optimizer, loss_function)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    224\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 225\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_epoch(dataset_train, model, optimizer, loss_function)\n\u001b[1;32m    226\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m validate_epoch(dataset_val, model, loss_function)\n\u001b[1;32m    227\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[96], line 203\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(dataset_train, model, optimizer, loss_function)\u001b[0m\n\u001b[1;32m    201\u001b[0m output, label_train \u001b[38;5;241m=\u001b[39m model(batch)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m#print(label_train.type(), output.type())\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(output, label_train)\n\u001b[1;32m    204\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    205\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[96], line 182\u001b[0m, in \u001b[0;36mloss_function\u001b[0;34m(output, target)\u001b[0m\n\u001b[1;32m    180\u001b[0m     target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mByteTensor)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m     target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mByteTensor)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m#plt.plot(target.cpu().detach().numpy())\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m#plt.show()\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m#print(target)\u001b[39;00m\n\u001b[1;32m    186\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(output, target)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "num_epochs = 10\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(lambda trial: objective(trial, dataset_train, dataset_val, num_epochs), n_trials=30)\n",
    "\n",
    "model = create_torch_model_CNN(trial)\n",
    "#model = model.to(device)\n",
    "\n",
    "learning_rate = 0.001\n",
    "torch.manual_seed(42)\n",
    "peram_to_optimize = model.parameters()\n",
    "optimizer = optim.Adam(peram_to_optimize, lr=learning_rate)\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15162\n"
     ]
    }
   ],
   "source": [
    "print(42*19*19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sk0rt3/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def test_func(model, dataset):\n",
    "    output = np.zeros(len(dataset))\n",
    "    label = np.zeros(len(dataset))\n",
    "    for batch in dataset:\n",
    "        model(batch)\n",
    "        \n",
    "for i in range(10):\n",
    "    test_func(model, dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu False\n",
      "torch.cuda.ByteTensor torch.FloatTensor\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 31\u001b[0m\n\u001b[1;32m     24\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice, dataset_train\u001b[38;5;241m.\u001b[39mis_cuda)\n\u001b[0;32m---> 31\u001b[0m train_losses, valid_losses \u001b[38;5;241m=\u001b[39m train_model(model, num_epochs, dataset_train, dataset_val, optimizer, loss_function)\n\u001b[1;32m     33\u001b[0m plot_losses(train_losses, valid_losses)\n",
      "Cell \u001b[0;32mIn[7], line 168\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, num_epochs, dataset_train, dataset_val, optimizer, loss_function)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    167\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 168\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_epoch(dataset_train, model, optimizer, loss_function)\n\u001b[1;32m    169\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m validate_epoch(dataset_val, model, loss_function)\n\u001b[1;32m    170\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[7], line 146\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(dataset_train, model, optimizer, loss_function)\u001b[0m\n\u001b[1;32m    144\u001b[0m output, label_train \u001b[38;5;241m=\u001b[39m model(batch)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mprint\u001b[39m(label_train\u001b[38;5;241m.\u001b[39mtype(), output\u001b[38;5;241m.\u001b[39mtype())\n\u001b[0;32m--> 146\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(output, label_train)\n\u001b[1;32m    147\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    148\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[7], line 131\u001b[0m, in \u001b[0;36mloss_function\u001b[0;34m(output, target)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_function\u001b[39m(output, target):\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(output, target)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3085\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)"
     ]
    }
   ],
   "source": [
    "model = create_torch_model_CNN(3).to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "dataset_train = train_loader.dataset[0]\n",
    "dataset_val = valid_loader.dataset[0]\n",
    "\n",
    "\n",
    "dataset_train = dataset_train.to(device)\n",
    "dataset_val = dataset_val.to(device)\n",
    "\n",
    "learning_rate = 0.001\n",
    "torch.manual_seed(42)\n",
    "peram_to_optimize = model.parameters()\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(peram_to_optimize, lr=learning_rate)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "\n",
    "print(next(model.parameters()).device, dataset_train.is_cuda)\n",
    "\n",
    "\n",
    "train_losses, valid_losses = train_model(model, num_epochs, dataset_train, dataset_val, optimizer, loss_function)\n",
    "\n",
    "plot_losses(train_losses, valid_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
