{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0   cluster     y     x          E   size\n",
      "0           0  004580_A   9.0  26.0   715232.0  108.0\n",
      "1           1  011701_G  15.0  17.0  1184202.0  179.0\n",
      "2           2  003882_A   3.0   3.0    31156.0    6.0\n",
      "3           3  009717_G   7.0  62.0  1423016.0  245.0\n",
      "4           4  005590_A  32.0  31.0  1014772.0  169.0\n",
      "   Unnamed: 0   cluster     y     x          E   size\n",
      "0           0  004580_A   9.0  26.0   715232.0  108.0\n",
      "1           1  011701_G  15.0  17.0  1184202.0  179.0\n",
      "2           2  003882_A   3.0   3.0    31156.0    6.0\n",
      "3           3  009717_G   7.0  62.0  1423016.0  245.0\n",
      "4           4  005590_A  32.0  31.0  1014772.0  169.0\n"
     ]
    }
   ],
   "source": [
    "meta_data = pd.read_csv('cluster_meta.csv')\n",
    "print(meta_data.head())\n",
    "\n",
    "def load_data(folder_path,meta_data):\n",
    "    data = []\n",
    "    for file in meta_data['cluster']:\n",
    "        file = file + '.csv'\n",
    "        file_path = os.path.join(folder_path,file)\n",
    "        cluster = pd.read_csv(file_path,header=None).values\n",
    "        cluster = cluster.flatten()\n",
    "        # cluster = np.append(cluster, meta_data[meta_data['cluster'] == file[:-4]][['y', 'x', 'E', 'size']].values.flatten())\n",
    "        data.append(cluster)\n",
    "    combined_array = np.stack(data,axis=0)\n",
    "    print('shape of combined array: ')\n",
    "    print(combined_array.shape)\n",
    "    return combined_array, meta_data[['y', 'x', 'E', 'size']].values\n",
    "\n",
    "\n",
    "meta_scaler = RobustScaler()\n",
    "meta_data_values = meta_scaler.fit_transform(meta_data[['y', 'x', 'E', 'size']])\n",
    "print(meta_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_and_pad_csvs(folder_path):\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    data_arrays = []\n",
    "    number_arrays = len(csv_files)\n",
    "    number_large = 0\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        data = pd.read_csv(file_path).values\n",
    "        data_arrays.append(data)\n",
    "    max_rows = max(array.shape[0] for array in data_arrays)\n",
    "    max_cols = max(array.shape[1] for array in data_arrays)\n",
    "    largest_array_index = max(range(len(data_arrays)), key=lambda i: data_arrays[i].shape[0] * data_arrays[i].shape[1])\n",
    "    print(f\"Index of the largest initial array: {largest_array_index}\")\n",
    "    padded_arrays = []\n",
    "    for array in data_arrays:\n",
    "        if array.shape[0] <= 64 and array.shape[1] <= 64:\n",
    "            padded_array = np.zeros((64, 64))\n",
    "            padded_array[:array.shape[0], :array.shape[1]] = array\n",
    "            padded_arrays.append(padded_array)\n",
    "            number_large += 1\n",
    "    print(f'Number of arrays that are smaller than 64x64: {number_large}, ({number_large/number_arrays*100:.2f}%)')\n",
    "    print(f'max rows: {max_rows}, max cols:{max_cols}')\n",
    "    combined_array = np.stack(padded_arrays, axis=0)\n",
    "    return combined_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of combined array: \n",
      "(3936, 4096)\n"
     ]
    }
   ],
   "source": [
    "class NumpyArrayDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data.astype(np.float32)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample  # Return only the sample and dummy label\n",
    "\n",
    "folder_path = 'clusters_colour_rotations_rescaled'\n",
    "combined_array, meta_data_values = load_data(folder_path,meta_data)\n",
    "\n",
    "scaler = QuantileTransformer()\n",
    "combined_array = scaler.fit_transform(combined_array)\n",
    "print(combined_array.shape)\n",
    "\n",
    "meta_data_values = scaler.fit_transform(meta_data_values)\n",
    "\n",
    "combined_array = np.append(combined_array, meta_data_values, axis=1)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "combined_array = np.shuffle(combined_array, axis=0)\n",
    "\n",
    "#meta_data_values = NumpyArrayDataset(meta_data_values, transform=transform)\n",
    "dataset = NumpyArrayDataset(combined_array, transform=transform)\n",
    "print(combined_array.shape)\n",
    "\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# test_size = len(dataset) - train_size\n",
    "# train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# print(meta_data_values.shape)\n",
    "\n",
    "\n",
    "# train_dataset, test_dataset, train_meta, test_meta = train_test_split(dataset, meta_data_values, test_size=0.2, random_state=42)\n",
    "# test_dataset = dataset\n",
    "\n",
    "# lazy, non-random split\n",
    "test_size = 0.2\n",
    "split_index = int(len(dataset) * (1 - test_size))\n",
    "train_dataset = dataset[:split_index]\n",
    "test_dataset = dataset[split_index:]\n",
    "train_meta = meta_data_values[:split_index]\n",
    "test_meta = meta_data_values[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3148, 4100])\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "valid_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(train_loader.dataset[0].shape)\n",
    "\n",
    "#train_meta = DataLoader(train_meta, batch_size=32, shuffle=False)\n",
    "#test_meta = DataLoader(test_meta, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_array.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.hist(combined_array[:,-1].flatten(), bins=np.linspace(0,1,1000),log=True)\n",
    "print(np.max(combined_array))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.imshow(combined_array[3929,:,:], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Var_Encoder(nn.Module):\n",
    "    def __init__(self, encoded_space_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        ### Convolutional section:\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, stride=1, padding=1),  # 1x256x256 -> 32x128x128\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, 3, stride=1, padding=1),  # 32x128x128 -> 64x64x64\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, return_indices=True)  # 64x64x64 -> 64x32x32\n",
    "        )\n",
    "        \n",
    "        # Flatten layer\n",
    "        self.flatten = nn.Flatten(start_dim=0)\n",
    " \n",
    "        # Linear section\n",
    "        self.encoder_lin_pic = nn.Sequential(\n",
    "            nn.Linear(65536, 128),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.encoder_lin_w_meta = nn.Sequential(\n",
    "            nn.Linear(128+4, encoded_space_dim),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        ### Variational part\n",
    "        self.variational_mean = nn.Linear(encoded_space_dim, encoded_space_dim)\n",
    "        self.variational_var = nn.Linear(encoded_space_dim, encoded_space_dim)\n",
    "\n",
    "    def reparameterization(self, mean, var):  # Stolen from MNIST example, device must be understood\n",
    "        epsilon = torch.randn_like(var).cpu() \n",
    "        z = mean + var * epsilon\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        meta = x[-4:]\n",
    "        x = x[:-4].unflatten(0, (1, 1, 64, 64))\n",
    "        # print('pass to encoder:', x.shape)\n",
    "        x, indices = self.encoder_cnn(x)  # Capture indices from MaxPool2d\n",
    "        # print('encoder output:', x.shape, 'indices', indices.shape)\n",
    "        x = self.flatten(x)\n",
    "        # print('flattened:', x.shape, meta.shape)\n",
    "        x = self.encoder_lin_pic(x)\n",
    "\n",
    "        y = torch.cat((x, meta), dim=0)\n",
    "        # print('concatenated:', y.shape)\n",
    "        x = self.encoder_lin_w_meta(y)\n",
    "        # print('linear output:', x.shape)\n",
    "        mean = self.variational_mean(x)\n",
    "        log_var = self.variational_var(x)\n",
    "        z = self.reparameterization(mean, torch.exp(0.5 * log_var))\n",
    "        return z, mean, log_var, indices\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, encoded_space_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder_lin = nn.Sequential(\n",
    "            nn.Linear(encoded_space_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 65536),  # Update to match your encoder's output\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.unflatten = nn.Unflatten(dim = 1, unflattened_size=(64, 32, 32))\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 64, 3, stride=1, padding=1),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.unpool = nn.MaxUnpool2d(2)\n",
    "        self.final_conv = nn.ConvTranspose2d(64, 1, 3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x, indices):\n",
    "        # print('test0', x.shape, indices.shape)\n",
    "        x = self.decoder_lin(x).reshape((1,65536))\n",
    "        # print('test1', x.shape)\n",
    "        x = self.unflatten(x)\n",
    "        # print('test2', x.shape)\n",
    "        x = self.decoder_conv(x)\n",
    "        # print('test3', x.shape)\n",
    "        # Here we assume the output size of the unpooling layer, which is the size of the maxpool input\n",
    "        # output_size = torch.Size([1, 64, 64, 64])\n",
    "        x = self.unpool(x, indices)#, output_size=output_size)\n",
    "        x = self.final_conv(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print((train_loader.dataset[0][0]))\n",
    "# x = train_loader.dataset[0][0]\n",
    "# print(x.shape)\n",
    "# x = x[:-4].reshape((1,1,64,64))\n",
    "\n",
    "\n",
    "# # encoder:\n",
    "# A = nn.Conv2d(1, 32, 3, stride=1, padding=1)\n",
    "# B = nn.Conv2d(32, 64 , 3, stride=1, padding=1)\n",
    "# C = nn.MaxPool2d(2)\n",
    "\n",
    "# # flatten\n",
    "# D = nn.Flatten()\n",
    "\n",
    "# # linear\n",
    "# E = nn.Linear(65536, 128)\n",
    "# F = nn.Linear(128, 10)\n",
    "\n",
    "# print(f'original data: {x.shape}')\n",
    "\n",
    "# print('encoder:')\n",
    "\n",
    "# print(f'Conv1: {A(x).shape}')\n",
    "# print(f'Conv2: {B(A(x)).shape}')\n",
    "# print(f'MaxPool: {C(B(A(x))).shape}')\n",
    "# print(f'Flatten: {D(C(B(A(x)))).shape}')\n",
    "# print(f'Linear1: {E(D(C(B(A(x))))).shape}')\n",
    "# print(f'Linear2: {F(E(D(C(B(A(x)))))).shape}')\n",
    "\n",
    "\n",
    "\n",
    "# print('decoder:')\n",
    "\n",
    "\n",
    "# x = train_loader.dataset[0][0]\n",
    "# x = x[:-4].reshape((1,1,64,64))\n",
    "# x = F(E(D(C(B(A(x))))))\n",
    "# # decoder:\n",
    "# G = nn.Linear(10, 128)\n",
    "# H = nn.Linear(128, 65536)\n",
    "# I = nn.Unflatten(dim=1, \n",
    "#         unflattened_size=(64, 32, 32))\n",
    "# J = nn.ConvTranspose2d(64, 64, 3, stride=1,padding = 1)#, output_padding=0)\n",
    "# # K = nn.BatchNorm2d(64)\n",
    "# L = nn.ConvTranspose2d(64, 1, 3, stride=1, padding = 1)#, padding=1, output_padding=1)\n",
    "# # M = nn.BatchNorm2d(8)\n",
    "# # N = nn.ConvTranspose2d(8, 1, 3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "# print(f'Linear1: {G(x).shape}')\n",
    "# print(f'Linear2: {H(G(x)).shape}')\n",
    "# print(f'Unflatten: {I(H(G(x))).shape}')\n",
    "# print(f'Conv1: {J(I(H(G(x)))).shape}')\n",
    "# print(f'BatchNorm: {K(J(I(H(G(x))))).shape}')\n",
    "# print(f'Conv2: {L(K(J(I(H(G(x)))))).shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data: torch.Size([1, 1, 64, 64])\n",
      "encoder:\n",
      "Conv1: torch.Size([1, 32, 64, 64])\n",
      "Conv2: torch.Size([1, 64, 64, 64])\n",
      "MaxPool: torch.Size([1, 64, 32, 32])\n",
      "Flatten: torch.Size([1, 65536])\n",
      "Linear1: torch.Size([1, 128])\n",
      "Linear2: torch.Size([1, 10])\n",
      "decoder:\n",
      "Linear1: torch.Size([1, 128])\n",
      "Linear2: torch.Size([1, 65536])\n",
      "Unflatten: torch.Size([1, 64, 32, 32])\n",
      "Conv1: torch.Size([1, 64, 32, 32])\n",
      "MaxUnpool: torch.Size([1, 64, 64, 64])\n",
      "Conv2: torch.Size([1, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "x = train_loader.dataset[0][0]\n",
    "x = x[:-4].reshape((1, 1, 64, 64))\n",
    "\n",
    "# Encoder:\n",
    "A = nn.Conv2d(1, 32, 3, stride=1, padding=1)\n",
    "B = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n",
    "C = nn.MaxPool2d(2, return_indices=True)  # Updated to return indices\n",
    "\n",
    "# Flatten\n",
    "D = nn.Flatten()\n",
    "\n",
    "# Linear\n",
    "E = nn.Linear(65536, 128)\n",
    "F = nn.Linear(128, 10)\n",
    "\n",
    "# Encoder forward pass\n",
    "conv1 = A(x)\n",
    "conv2 = B(conv1)\n",
    "maxpool, indices = C(conv2)  # Capture indices for unpooling\n",
    "# print(indices.shape)\n",
    "# print(indices)\n",
    "flattened = D(maxpool)\n",
    "linear1 = E(flattened)\n",
    "linear2 = F(linear1)\n",
    "\n",
    "print(f'original data: {x.shape}')\n",
    "\n",
    "print('encoder:')\n",
    "print(f'Conv1: {conv1.shape}')\n",
    "print(f'Conv2: {conv2.shape}')\n",
    "print(f'MaxPool: {maxpool.shape}')\n",
    "print(f'Flatten: {flattened.shape}')\n",
    "print(f'Linear1: {linear1.shape}')\n",
    "print(f'Linear2: {linear2.shape}')\n",
    "\n",
    "# Decoder:\n",
    "G = nn.Linear(10, 128)\n",
    "H = nn.Linear(128, 65536)\n",
    "I = nn.Unflatten(dim=1, unflattened_size=(64, 32, 32))\n",
    "J = nn.ConvTranspose2d(64, 64, 3, stride=1, padding=1)\n",
    "K = nn.MaxUnpool2d(2)\n",
    "L = nn.ConvTranspose2d(64, 1, 3, stride=1, padding=1)\n",
    "\n",
    "# Decoder forward pass\n",
    "x = linear2\n",
    "linear1 = G(x)\n",
    "linear2 = H(linear1)\n",
    "unflattened = I(linear2)\n",
    "conv1_trans = J(unflattened)\n",
    "unpool = K(conv1_trans, indices)  # Use indices here\n",
    "\n",
    "\n",
    "print('decoder:')\n",
    "print(f'Linear1: {linear1.shape}')\n",
    "print(f'Linear2: {linear2.shape}')\n",
    "print(f'Unflatten: {unflattened.shape}')\n",
    "print(f'Conv1: {conv1_trans.shape}')\n",
    "print(f'MaxUnpool: {unpool.shape}')\n",
    "\n",
    "conv2_trans = L(unpool)\n",
    "\n",
    "print(f'Conv2: {conv2_trans.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and training parameters:\n",
    "def loss_fn(recon_x, x, mean, log_var):\n",
    "    #MSE = nn.MSELoss()(recon_x, x) \n",
    "    BCE = nn.BCELoss(reduction='sum')(recon_x, x)\n",
    "    KLD = - 0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "    #print(MSE, KLD)\n",
    "    lambda_= 1\n",
    "    return BCE + lambda_ * KLD\n",
    "\n",
    "# loss_fn = nn.MSELoss()\n",
    "\n",
    "lr = 0.01\n",
    "torch.manual_seed(42)\n",
    "\n",
    "d = 28\n",
    "encoder = Var_Encoder(encoded_space_dim=d)\n",
    "# var_encoder = Var_Encoder(encoded_space_dim=d)\n",
    "decoder = Decoder(encoded_space_dim=d)\n",
    "params_to_optimize = [\n",
    "    {'params': encoder.parameters()},\n",
    "    {'params': decoder.parameters()}\n",
    "]\n",
    "\n",
    "optim = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-05)\n",
    "\n",
    "def train_epoch(encoder, decoder, dataloader, loss_fn, optimizer):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    train_loss = []\n",
    "    for image_batch in dataloader.dataset[0]:\n",
    "        #encoded_data = encoder(image_batch)\n",
    "        # print('hi')\n",
    "        encoded_data,mean,var, indices = encoder(image_batch) #VAE\n",
    "        # print('hello')\n",
    "        decoded_data = decoder(encoded_data, indices)\n",
    "        # loss = loss_fn(decoded_data, image_batch[:-4].reshape(decoded_data.shape))\n",
    "        # print('no error so far')\n",
    "        # print(image_batch[:-4].reshape(decoded_data.shape))\n",
    "        loss = loss_fn(decoded_data, image_batch[:-4].reshape(decoded_data.shape),mean,var) #VAE\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.detach().cpu().numpy())\n",
    "    return np.mean(train_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(encoder, decoder, dataloader, loss_fn):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        conc_out = []\n",
    "        conc_label = []\n",
    "        mean_out = []\n",
    "        var_out = []\n",
    "        for image_batch in dataloader.dataset[0]:\n",
    "            #encoded_data = encoder(image_batch)\n",
    "            encoded_data,mean,var, indices = encoder(image_batch) #VAE\n",
    "            decoded_data = decoder(encoded_data,indices)\n",
    "            conc_out.append(decoded_data.cpu())\n",
    "            conc_label.append(image_batch[:-4].reshape(decoded_data.shape).cpu())\n",
    "            mean_out.append(mean.cpu())\n",
    "            var_out.append(var.cpu())\n",
    "        conc_out = torch.cat(conc_out)\n",
    "        conc_label = torch.cat(conc_label)\n",
    "        mean = torch.cat(mean_out) #VAE\n",
    "        var = torch.cat(var_out) #VAE\n",
    "        # val_loss = loss_fn(conc_out, conc_label)\n",
    "        val_loss = loss_fn(conc_out, conc_label,mean,var) #VAE\n",
    "    return val_loss.data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "diz_loss = {'train_loss': [], 'val_loss': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 \t train loss: 1.77018e+05 \t val loss: 6.07729e+05 \t time: 730.235910654068\n",
      "Elapsed time: 730.235910654068\n",
      "Epoch 2/15 \t train loss: 9.08e+02 \t Delta: 1.76110e+05 \t val loss: 6.00501e+05 \t Delta: 7.23e+03 \t time: 992.1826002597809\n",
      "Elapsed time: 992.1826002597809\n",
      "Epoch 3/15 \t train loss: 9.24e+02 \t Delta: -1.51145e+01 \t val loss: 6.17173e+05 \t Delta: -1.67e+04 \t time: 1086.2659907341003\n",
      "Elapsed time: 1086.2659907341003\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      3\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 4\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m test_epoch(encoder, decoder, test_loader, loss_fn)\n\u001b[0;32m      6\u001b[0m     elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[1;32mIn[82], line 42\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(encoder, decoder, dataloader, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     40\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     41\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 42\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     train_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(train_loss)\n",
      "File \u001b[1;32mc:\\Users\\Jens\\anaconda3\\envs\\appmlenv\\Lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jens\\anaconda3\\envs\\appmlenv\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\Jens\\anaconda3\\envs\\appmlenv\\Lib\\site-packages\\torch\\optim\\adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    158\u001b[0m         group,\n\u001b[0;32m    159\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    164\u001b[0m         state_steps)\n\u001b[1;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Jens\\anaconda3\\envs\\appmlenv\\Lib\\site-packages\\torch\\optim\\adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jens\\anaconda3\\envs\\appmlenv\\Lib\\site-packages\\torch\\optim\\adam.py:441\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m--> 441\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(encoder, decoder, train_loader, loss_fn, optim)\n",
    "    val_loss = test_epoch(encoder, decoder, test_loader, loss_fn)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if epoch != 0:\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs} \\t train loss: {train_loss:.2e} \\t Delta: {diz_loss['train_loss'][-1]-train_loss:.5e} \\t val loss: {val_loss:.5e} \\t Delta: {diz_loss['val_loss'][-1]-val_loss:.2e} \\t time: {elapsed_time}')\n",
    "    else:\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs} \\t train loss: {train_loss:.5e} \\t val loss: {val_loss:.5e} \\t time: {elapsed_time}')\n",
    "    print(f'Elapsed time: {elapsed_time}')\n",
    "    diz_loss['train_loss'].append(train_loss)\n",
    "    diz_loss['val_loss'].append(val_loss)\n",
    "    #plot_ae_outputs(encoder, decoder, n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots()\n\u001b[0;32m      2\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_yscale(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ax.set_xscale('log')\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_yscale('log')\n",
    "# ax.set_xscale('log')\n",
    "ax.plot(diz_loss['train_loss'], label='train loss')\n",
    "# ax.plot(diz_loss['val_loss'], label='val loss')\n",
    "ax.legend()\n",
    "# ax.set_xlim(0, 50)\n",
    "\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (decoder_lin): Sequential(\n",
       "    (0): Linear(in_features=28, out_features=128, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=128, out_features=1296, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "  )\n",
       "  (unflatten): Unflatten(dim=1, unflattened_size=(16, 9, 9))\n",
       "  (decoder_conv): Sequential(\n",
       "    (0): ConvTranspose2d(16, 16, kernel_size=(16, 16), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): ConvTranspose2d(16, 1, kernel_size=(8, 8), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sumf = 0\n",
    "# for i in range(6400):\n",
    "#     img = test_dataset[i][0].unsqueeze(0).numpy()\n",
    "#     suming = np.sum(img)\n",
    "#     if suming>sumf:\n",
    "#         sumf = sumimg\n",
    "#         idx = i\n",
    "# print(i)\n",
    "# print(sumf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAEjCAYAAACSDWOaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAvMklEQVR4nO3deXxTdbo/8E/XUFqabtC0dGWRgiyySImA4FCnLwVlKSio2KpXBcpSZRzl+gMddCzCeIfBUUSvgiMqYzuCMHOVwSJldCpCBdlLgSIVmlSKTcpSCs3z+8Pbc3tIi0lJT5L28369ntcr/X5PznlyJI9PzpL4iIiAiIiISCO+7k6AiIiI2hc2H0RERKQpNh9ERESkKTYfREREpCk2H0RERKQpNh9ERESkKTYfREREpCk2H0RERKQpNh9ERESkKTYfXur555+Hj49Pi567Zs0a+Pj44MSJE65NqpETJ07Ax8cHa9asueZy27Ztg4+PD7Zt29ZquRARNYV1yn3YfGjswIEDeOCBB9C1a1fodDrExsbi/vvvx4EDB9ydGhE5oKF5bwh/f3907doVWVlZOHXqlLvTc7nXX3/9F//n3B5yINfy4W+7aOfjjz/GtGnTEBERgUceeQTJyck4ceIE3n77bVRVVWHdunWYOHGiQ+u6cuUKrly5gg4dOjidR319PS5fvgydTtfioye/5MSJE0hOTsbq1auRlZXV7HI2mw11dXUIDAyEry97YfJ8a9aswUMPPYTFixcjOTkZtbW1+Prrr7FmzRokJSVh//79LXpfeqq+ffsiKirKrZ/6WysH1in38Xd3Au3FsWPHMH36dHTr1g3bt29H586dlbl58+Zh5MiRmD59Ovbu3Ytu3bo1u57z588jODgY/v7+8Pdv2X8+Pz8/+Pn5tei5rubr69umCjW1H3fccQeGDBkCAPiP//gPREVF4eWXX8bGjRtxzz33uDk792ioT20N65TrsYXTyLJly3DhwgW8+eabqsYDAKKiorBq1SqcP38eS5cuVcYbrus4ePAg7rvvPoSHh2PEiBGqucYuXryIuXPnIioqCp06dcLdd9+NU6dOwcfHB88//7yyXFPXfCQlJWHcuHH48ssvMXToUHTo0AHdunXDX/7yF9U2zp49i9/85jfo168fQkJCEBoaijvuuAPfffddi/ZLU+dSR48ejb59+2Lv3r0YNWoUOnbsiB49eiA/Px8AUFhYiNTUVAQFBaFXr174/PPPVev8/vvvMWvWLPTq1QtBQUGIjIzElClTmrzGpWEbQUFBiIuLw4svvojVq1c3eU3Mp59+ipEjRyI4OBidOnXC2LFjebqMFCNHjgTw8weNxg4fPozJkycjIiICHTp0wJAhQ7Bx40a751dXV+OJJ55AUlISdDod4uLi8OCDD+LMmTPKMpWVlXjkkUcQHR2NDh06YMCAAXj33XdV62m4juEPf/gD3nzzTXTv3h06nQ4333wzdu7cqVrWZDLhoYceQlxcHHQ6HWJiYjB+/Hjl335SUhIOHDiAwsJC5TTT6NGjAfxfHSksLMSsWbPQpUsXxMXFAQCysrKQlJRk9xqbu1Zt7dq1GDp0KDp27Ijw8HDceuut+Oc///mLOTTst5ycHMTHx0On06FHjx54+eWXYbPZ7PZvVlYW9Ho9wsLCkJmZierqartcmsI65Xo88qGRTZs2ISkpSSlQV7v11luRlJSEf/zjH3ZzU6ZMQc+ePfHSSy/hWmfJsrKy8NFHH2H69OkYNmwYCgsLMXbsWIdzPHr0KCZPnoxHHnkEmZmZeOedd5CVlYXBgwfjxhtvBAAcP34cGzZswJQpU5CcnAyz2YxVq1Zh1KhROHjwIGJjYx3e3rX89NNPGDduHKZOnYopU6Zg5cqVmDp1Kt5//33k5ORgxowZuO+++7Bs2TJMnjwZ5eXl6NSpEwBg586d+Pe//42pU6ciLi4OJ06cwMqVKzF69GgcPHgQHTt2BACcOnUKt912G3x8fLBgwQIEBwfjv//7v6HT6ezyee+995CZmYn09HS8/PLLuHDhAlauXIkRI0Zg9+7dTRZaal8a/icQHh6ujB04cADDhw9H165d8cwzzyA4OBgfffQRJkyYgL/97W/KadZz585h5MiROHToEB5++GEMGjQIZ86cwcaNG/HDDz8gKioKFy9exOjRo3H06FHMnj0bycnJyMvLQ1ZWFqqrqzFv3jxVPh988AFqamrw+OOPw8fHB0uXLsWkSZNw/PhxBAQEAAAyMjJw4MABzJkzB0lJSaisrMSWLVtw8uRJJCUlYfny5ZgzZw5CQkLw7LPPAgCio6NV25k1axY6d+6MRYsW4fz5807vt9/97nd4/vnnccstt2Dx4sUIDAzEjh07sHXrVvz617++Zg4XLlzAqFGjcOrUKTz++ONISEjAv//9byxYsAAVFRVYvnw5AEBEMH78eHz55ZeYMWMGevfujfXr1yMzM9PpfBtjnboOQq2uurpaAMj48eOvudzdd98tAMRqtYqIyHPPPScAZNq0aXbLNsw1KC4uFgCSk5OjWi4rK0sAyHPPPaeMrV69WgBIWVmZMpaYmCgAZPv27cpYZWWl6HQ6mT9/vjJWW1sr9fX1qm2UlZWJTqeTxYsXq8YAyOrVq6/5mr/44gsBIF988YUyNmrUKAEgH3zwgTJ2+PBhASC+vr7y9ddfK+ObN2+2286FCxfstlNUVCQA5C9/+YsyNmfOHPHx8ZHdu3crY1VVVRIREaHaPzU1NRIWFiaPPvqoap0mk0n0er3dOLVtDe+fzz//XH788UcpLy+X/Px86dy5s+h0OikvL1eWHTNmjPTr109qa2uVMZvNJrfccov07NlTGVu0aJEAkI8//thuezabTUREli9fLgBk7dq1ylxdXZ0YjUYJCQlR6kbDey8yMlLOnj2rLPvJJ58IANm0aZOIiPz0008CQJYtW3bN13vjjTfKqFGjmt0PI0aMkCtXrqjmMjMzJTEx0e45V9et0tJS8fX1lYkTJ9rVlYbXfa0cXnjhBQkODpYjR46oxp955hnx8/OTkydPiojIhg0bBIAsXbpUWebKlSsycuRI1ik34WkXDdTU1ACA0vE2p2HearWqxmfMmPGL2/jss88A/PwppLE5c+Y4nGefPn1UR2Y6d+6MXr164fjx48qYTqdTLriqr69HVVUVQkJC0KtXL3z77bcOb+uXhISEYOrUqcrfvXr1QlhYGHr37o3U1FRlvOFx4xyDgoKUx5cvX0ZVVRV69OiBsLAwVY6fffYZjEYjbrrpJmUsIiIC999/vyqXLVu2oLq6GtOmTcOZM2eU8PPzQ2pqKr744guXvW7yHmlpaejcuTPi4+MxefJkBAcHY+PGjcqph7Nnz2Lr1q245557UFNTo/y7qaqqQnp6OkpLS5W7Y/72t79hwIABTV5w3nCa4n/+539gMBgwbdo0ZS4gIABz587FuXPnUFhYqHrevffeqzoK0/DebnivBAUFITAwENu2bcNPP/3U4v3w6KOPtvgasg0bNsBms2HRokV2F3I6cjF8Xl4eRo4cifDwcNV7My0tDfX19di+fTuAn/edv78/Zs6cqTzXz8/PqfrYFNapluNpFw00NBUNTUhzmmtSkpOTf3Eb33//PXx9fe2W7dGjh8N5JiQk2I2Fh4erCpPNZsOf/vQnvP766ygrK0N9fb0yFxkZ6fC2fklcXJxd8dHr9YiPj7cbA6DK8eLFi8jNzcXq1atx6tQp1akqi8WiPP7+++9hNBrttn31PistLQUA/OpXv2oy19DQUEdeErUxr732Gm644QZYLBa888472L59u+pQ+NGjRyEiWLhwIRYuXNjkOiorK9G1a1ccO3YMGRkZ19ze999/j549e9r9T7p3797KfGNXv58bGpGG94pOp8PLL7+M+fPnIzo6GsOGDcO4cePw4IMPwmAwOLAHfuZIfWrOsWPH4Ovriz59+rTo+aWlpdi7d6/ddXQNKisrAfy8b2JiYhASEqKa79WrV4u224B1quXYfGhAr9cjJiYGe/fuveZye/fuRdeuXe3+kTTukFtTc59eGr8pXnrpJSxcuBAPP/wwXnjhBURERMDX1xc5OTl2F3i1Ri6O5DhnzhysXr0aOTk5MBqN0Ov18PHxwdSpU1uUY8Nz3nvvvSaLckvvOiLvNnToUOVulwkTJmDEiBG47777UFJSgpCQEOXfzW9+8xukp6c3uQ5nPhw4y5H3Sk5ODu666y5s2LABmzdvxsKFC5Gbm4utW7di4MCBDm2nqfrU3FGLxh9WXMFms+H222/Hb3/72ybnb7jhBpdu72qsUy3nWdm0YePGjcNbb72FL7/8UrljpbF//etfOHHiBB5//PEWrT8xMRE2mw1lZWXo2bOnMn706NEW59yU/Px83HbbbXj77bdV49XV1YiKinLptloqPz8fmZmZeOWVV5Sx2tpauyvbExMTm9w/V491794dANClSxekpaW5PmHyen5+fsjNzcVtt92GP//5z3jmmWeUW+YDAgJ+8d9N9+7dsX///msuk5iYiL1798Jms6mOfhw+fFiZb4nu3btj/vz5mD9/PkpLS3HTTTfhlVdewdq1awE4dvrjauHh4U3eSXL10Znu3bvDZrPh4MGDqtMKV2suh+7du+PcuXO/uH8TExNRUFCAc+fOqY5+lJSUXPN5ram91yle86GRp556CkFBQXj88cdRVVWlmjt79ixmzJiBjh074qmnnmrR+hs+Wb3++uuq8VdffbVlCTfDz8/P7o6bvLw8j/pmx6ZyfPXVV+0+daWnp6OoqAh79uxRxs6ePYv333/fbrnQ0FC89NJLuHz5st32fvzxR9clT15r9OjRGDp0KJYvX47a2lp06dIFo0ePxqpVq1BRUWG3fON/NxkZGfjuu++wfv16u+Ua/i3feeedMJlM+Otf/6rMXblyBa+++ipCQkIwatQop/K9cOECamtrVWPdu3dHp06dcOnSJWUsODjY4VtSG6/HYrGojvZWVFTYvb4JEybA19cXixcvtvu03/g93FwO99xzD4qKirB582a7uerqaly5cgXAz/vuypUrWLlypTJfX1/v8vrojPZep3jkQyM9e/bEu+++i/vvvx/9+vWz+4bTM2fO4MMPP1S6V2cNHjwYGRkZWL58OaqqqpRbbY8cOQKgZZ9emjJu3DgsXrwYDz30EG655Rbs27cP77///jW/GE1r48aNw3vvvQe9Xo8+ffqgqKgIn3/+ud01Kb/97W+xdu1a3H777ZgzZ45yC1tCQgLOnj2r7LPQ0FCsXLkS06dPx6BBgzB16lR07twZJ0+exD/+8Q8MHz4cf/7zn93xUsnDPPXUU5gyZQrWrFmDGTNm4LXXXsOIESPQr18/PProo+jWrRvMZjOKiorwww8/KN+P89RTTyE/Px9TpkzBww8/jMGDB+Ps2bPYuHEj3njjDQwYMACPPfYYVq1ahaysLBQXFyMpKQn5+fn46quvsHz58l+8oP1qR44cwZgxY3DPPfegT58+8Pf3x/r162E2m1UXUQ4ePBgrV67Eiy++iB49eqBLly7NXlfQYOrUqXj66acxceJEzJ07V7nl84YbblBdTNmjRw88++yzeOGFFzBy5EhMmjQJOp0OO3fuRGxsLHJzc6+Zw1NPPYWNGzdi3LhxytcCnD9/Hvv27UN+fj5OnDiBqKgo3HXXXRg+fDieeeYZnDhxAn369MHHH3+surZCa+2+TrnnJpv2a+/evTJt2jSJiYmRgIAAMRgMMm3aNNm3b5/dsg23pf3444/NzjV2/vx5yc7OloiICAkJCZEJEyZISUmJAJAlS5YoyzV3q+3YsWPttjNq1CjVLW61tbUyf/58iYmJkaCgIBk+fLgUFRXZLXe9t9reeOONdss2lyMAyc7OVv7+6aef5KGHHpKoqCgJCQmR9PR0OXz4sCQmJkpmZqbqubt375aRI0eKTqeTuLg4yc3NlRUrVggAMZlMdrmmp6eLXq+XDh06SPfu3SUrK0t27dp1zddIbUvD+2fnzp12c/X19dK9e3fp3r27cvvpsWPH5MEHHxSDwSABAQHStWtXGTdunOTn56ueW1VVJbNnz5auXbtKYGCgxMXFSWZmppw5c0ZZxmw2K/+2AwMDpV+/fnbvsYb3XlO30KLRbfdnzpyR7OxsSUlJkeDgYNHr9ZKamiofffSR6jkmk0nGjh0rnTp1EgDK+/xa+0FE5J///Kf07dtXAgMDpVevXrJ27dom65aIyDvvvCMDBw4UnU4n4eHhMmrUKNmyZcsv5iDy8y2mCxYskB49ekhgYKBERUXJLbfcIn/4wx+krq5OtX+nT58uoaGhotfrZfr06bJ7927WKTfhb7u0cXv27MHAgQOxdu1au1uzqGk5OTlYtWoVzp075zFfQ09E1Ji31yle89GGXLx40W5s+fLl8PX1xa233uqGjDzf1fusqqoK7733HkaMGOGVb2gianvaYp3iNR9tyNKlS1FcXIzbbrsN/v7++PTTT/Hpp5/iscces7vvnH5mNBoxevRo9O7dG2azGW+//TasVmuz38tARKS1tlineNqlDdmyZQt+97vf4eDBgzh37hwSEhIwffp0PPvssx53j7en+M///E/k5+fjhx9+gI+PDwYNGoTnnnvOK25VI6L2oS3WKTYfREREpCle80FERESaarXm47XXXkNSUhI6dOiA1NRUfPPNN621KSJqI1g3iNqHVjnt8te//hUPPvgg3njjDaSmpmL58uXIy8tDSUkJunTpcs3n2mw2nD59Gp06dXLZF2MRkXNEBDU1NYiNjbX7IbPWcj11A2DtIHI3p+pGa3x5yNChQ1VfplJfXy+xsbGSm5v7i88tLy8XAAwGwwOivLy8NUpEk66nboiwdjAYnhKO1A2Xf6Spq6tDcXGx6ipcX19fpKWloaioyG75S5cuwWq1KiG8/pXIYzj7ld0t5WzdAFg7iDyVI3XD5c3HmTNnUF9fj+joaNV4dHQ0TCaT3fK5ubnQ6/VKJCQkuDolImohrU5fOFs3ANYOIk/lSN1w+90uCxYsgMViUaK8vNzdKRGRF2DtIPJeLv/mqaioKPj5+cFsNqvGzWYzDAaD3fI6nQ46nc7VaRCRF3G2bgCsHUTezOVHPgIDAzF48GAUFBQoYzabDQUFBTAaja7eHBG1AawbRO1MCy9Mv6Z169aJTqeTNWvWyMGDB+Wxxx6TsLAwu5/+bYrFYnH7lboMBuPnsFgsrVEimnQ9dUOEtYPB8JRwpG60yg9+3Hvvvfjxxx+xaNEimEwm3HTTTfjss8/sLiYjImrAukHUfnjcb7tYrVbo9Xp3p0FEACwWC0JDQ92dhkNYO4g8gyN1w+13uxAREVH7wuaDiIiINMXmg4iIiDTF5oOIiIg0xeaDiIiINMXmg4iIiDTF5oOIiIg0xeaDiIiINMXmg4iIiDTF5oOIiIg0xeaDiIiINMXmg4iIiDTF5oOIiIg0xeaDiIiINMXmg4iIiDTF5oOIiIg0xeaDiIiINMXmg4iIiDTF5oOIiIg0xeaDiIiINMXmg4iIiDTF5oOIiIg0xeaDiIiINMXmg4iIiDTldPOxfft23HXXXYiNjYWPjw82bNigmhcRLFq0CDExMQgKCkJaWhpKS0tdlS8ReSHWDSJqzOnm4/z58xgwYABee+21JueXLl2KFStW4I033sCOHTsQHByM9PR01NbWXneyROSdWDeISEWuAwBZv3698rfNZhODwSDLli1Txqqrq0Wn08mHH37o0DotFosAYDAYHhAWi+V6SkSTANfXDRHWDgbDU8KRuuHSaz7KyspgMpmQlpamjOn1eqSmpqKoqKjJ51y6dAlWq1UVRNR+tKRuAKwdRN7Mpc2HyWQCAERHR6vGo6Ojlbmr5ebmQq/XKxEfH+/KlIjIw7WkbgCsHUTezO13uyxYsAAWi0WJ8vJyd6dERF6AtYPIe7m0+TAYDAAAs9msGjebzcrc1XQ6HUJDQ1VBRO1HS+oGwNpB5M1c2nwkJyfDYDCgoKBAGbNardixYweMRqMrN0VEbQTrBlH74+/sE86dO4ejR48qf5eVlWHPnj2IiIhAQkICcnJy8OKLL6Jnz55ITk7GwoULERsbiwkTJrgybyLyIqwbRKTi1D1yIvLFF180eWtNZmamctvcwoULJTo6WnQ6nYwZM0ZKSkp4uxyD4YXhqlttW7tusHYwGJ4TjtQNHxEReBCr1Qq9Xu/uNIgIgMVi8ZprKVg7iDyDI3XD7Xe7EBERUfvC5oOIiIg0xeaDiIiINMXmg4iIiDTF5oOIiIg0xeaDiIiINMXmg4iIiDTF5oOIiIg0xeaDiIiINMXmg4iIiDTF5oOIiIg0xeaDiIiINMXmg4iIiDTF5oOIiIg0xeaDiIiINMXmg4iIiDTF5oOIiIg0xeaDiIiINMXmg4iIiDTF5oOIiIg0xeaDiIiINMXmg4iIiDTF5oOIiIg0xeaDiIiINOVU85Gbm4ubb74ZnTp1QpcuXTBhwgSUlJSolqmtrUV2djYiIyMREhKCjIwMmM1mlyZNRN6FtYOIGnOq+SgsLER2dja+/vprbNmyBZcvX8avf/1rnD9/XlnmiSeewKZNm5CXl4fCwkKcPn0akyZNcnniROQ9WDuISEWuQ2VlpQCQwsJCERGprq6WgIAAycvLU5Y5dOiQAJCioiKH1mmxWAQAg8HwgLBYLNdTIlg7GIx2GI7Ujeu65sNisQAAIiIiAADFxcW4fPky0tLSlGVSUlKQkJCAoqKiJtdx6dIlWK1WVRBR28baQdS+tbj5sNlsyMnJwfDhw9G3b18AgMlkQmBgIMLCwlTLRkdHw2QyNbme3Nxc6PV6JeLj41uaEhF5AdYOImpx85GdnY39+/dj3bp115XAggULYLFYlCgvL7+u9RGRZ2PtICL/ljxp9uzZ+Pvf/47t27cjLi5OGTcYDKirq0N1dbXqE4zZbIbBYGhyXTqdDjqdriVpEJGXYe0gIgBw6oJTm80m2dnZEhsbK0eOHLGbb7hoLD8/Xxk7fPiwALxojMHwxnDVBaesHQxG+wlH6oZTzcfMmTNFr9fLtm3bpKKiQokLFy4oy8yYMUMSEhJk69atsmvXLjEajWI0Gh3eBgsIg+E54armg7WDwWg/4fLmo7kNrV69Wlnm4sWLMmvWLAkPD5eOHTvKxIkTpaKiggWEwfDCcFXz0dz6WTsYjLYXjtQNn/8tDB7DarVCr9e7Ow0iws+3xIaGhro7DYewdhB5BkfqBn/bhYiIiDTF5oOIiIg0xeaDiIiINMXmg4iIiDTF5oOIiIg0xeaDiIiINMXmg4iIiDTF5oOIiIg0xeaDiIiINMXmg4iIiDTF5oOIiIg0xeaDiIiINOXv7gTIe/j6qntVm83mpkyIiMib8cgHERERaYrNBxEREWmKp13omrp166Y87t+/v2ruX//6l/K4qqpKs5yIiMi78cgHERERaYrNBxEREWmKzQcRERFpitd8kIqfn5/q7zvvvFN5/MADD6jm9Hq98vjjjz9WzdXU1LRCdkRE1BbwyAcRERFpis0HERERaYqnXUjl6m8xjYyMVB6npqaq5sLDw5XHZrNZNffZZ5+1QnZERNQW8MgHERERacqp5mPlypXo378/QkNDERoaCqPRiE8//VSZr62tRXZ2NiIjIxESEoKMjAy7T8RE1P6wdhBRY041H3FxcViyZAmKi4uxa9cu/OpXv8L48eNx4MABAMATTzyBTZs2IS8vD4WFhTh9+jQmTZrUKokTkfdg7SCixnxERK5nBREREVi2bBkmT56Mzp0744MPPsDkyZMBAIcPH0bv3r1RVFSEYcOGObQ+q9WquoWTtBUUFKT6OysrS3n82GOPqeYa/6rt//t//0811/hTLXkvi8WC0NDQVlk3awdR2+RI3WjxNR/19fVYt24dzp8/D6PRiOLiYly+fBlpaWnKMikpKUhISEBRUVGz67l06RKsVqsqiKjtYu0gIqebj3379iEkJAQ6nQ4zZszA+vXr0adPH5hMJgQGBiIsLEy1fHR0NEwmU7Pry83NhV6vVyI+Pt7pF0FEno+1g4gaOH2rba9evbBnzx5YLBbk5+cjMzMThYWFLU5gwYIFePLJJ5W/rVYri4gbNT6VAgCnT59WHv/Xf/2Xaq6srEx5vHv37tZNjLweawcRNXC6+QgMDESPHj0AAIMHD8bOnTvxpz/9Cffeey/q6upQXV2t+gRjNpthMBiaXZ9Op4NOp3M+cyLyKqwdRNTgur/nw2az4dKlSxg8eDACAgJQUFCgzJWUlODkyZMwGo3XuxkiamNYO4jaL6eOfCxYsAB33HEHEhISUFNTgw8++ADbtm3D5s2bodfr8cgjj+DJJ59EREQEQkNDMWfOHBiNRoevVieitom1g4gac6r5qKysxIMPPoiKigro9Xr0798fmzdvxu233w4A+OMf/whfX19kZGTg0qVLSE9Px+uvv94qiVPruHLliurvxl/0lJSUpJo7dOiQ8vjy5cutmhd5N9YOImrsur/nw9V4r757+fn5qf6++eablcdX/7bLV199pTzeu3evaq6urq4VsiOtteb3fLgaaweRZ2jV7/kgIiIiagk2H0RERKQpp2+1pbatvr5e9ffhw4eVx1ff9tivXz/lcadOnVRzja8HAYCzZ882u42r/yYioraNRz6IiIhIU2w+iIiISFM87ULXVF1drTzevn27am7AgAHK46tPyYSEhKj+bvyjXzU1Naq5xqd2Lly40OJciYjIO/DIBxEREWmKzQcRERFpis0HERERaYrXfJDDGt8uC6i/4TQyMlI1d/VPm6ekpCiPAwMDVXONr/M4cuSIas5ms7UsWSIi8lg88kFERESaYvNBREREmuJpF2qxxj8eV1FRoZqrqqpS/V1ZWak8TkhIUM01/jE7X191P8zTLkREbQ+PfBAREZGm2HwQERGRpth8EBERkaZ4zQe1isbXgwDAiRMnlMdms1k15+Pjozy+cuVKq+ZFRETuxyMfREREpCk2H0RERKQpnnYhzV28eNHdKRARkRvxyAcRERFpis0HERERaYrNBxEREWmKzQcRERFpis0HERERaeq6mo8lS5bAx8cHOTk5ylhtbS2ys7MRGRmJkJAQZGRk2H2pFBG1X6wbRNTi5mPnzp1YtWoV+vfvrxp/4oknsGnTJuTl5aGwsBCnT5/GpEmTrjtRIvJ+rBtEBACQFqipqZGePXvKli1bZNSoUTJv3jwREamurpaAgADJy8tTlj106JAAkKKioibXVVtbKxaLRYny8nIBwGAwPCAsFktLSkSr1w3WDgbDc8ORutGiIx/Z2dkYO3Ys0tLSVOPFxcW4fPmyajwlJQUJCQkoKipqcl25ubnQ6/VKxMfHtyQlIvJwrqwbAGsHkTdzuvlYt24dvv32W+Tm5trNmUwmBAYGIiwsTDUeHR0Nk8nU5PoWLFgAi8WiRHl5ubMpEZGHc3XdAFg7iLyZU1+vXl5ejnnz5mHLli3o0KGDSxLQ6XTQ6XQuWRcReZ7WqBsAaweRN3PqyEdxcTEqKysxaNAg+Pv7w9/fH4WFhVixYgX8/f0RHR2Nuro6VFdXq55nNpthMBhcmTcReQnWDSK6mlNHPsaMGYN9+/apxh566CGkpKTg6aefRnx8PAICAlBQUICMjAwAQElJCU6ePAmj0ei6rInIa7BuENHVnGo+OnXqhL59+6rGgoODERkZqYw/8sgjePLJJxEREYHQ0FDMmTMHRqMRw4YNc13WROQ1WDeI6GpONR+O+OMf/whfX19kZGTg0qVLSE9Px+uvv+7qzRBRG8K6QdS++IiIuDuJxqxWK/R6vbvTICIAFosFoaGh7k7DIawdRJ7BkbrB33YhIiIiTbH5ICIiIk2x+SAiIiJNsfkgIiIiTbH5ICIiIk2x+SAiIiJNsfkgIiIiTbH5ICIiIk2x+SAiIiJNsfkgIiIiTbH5ICIiIk2x+SAiIiJNsfkgIiIiTbH5ICIiIk2x+SAiIiJNsfkgIiIiTbH5ICIiIk2x+SAiIiJNsfkgIiIiTbH5ICIiIk2x+SAiIiJNsfkgIiIiTbH5ICIiIk2x+SAiIiJNOdV8PP/88/Dx8VFFSkqKMl9bW4vs7GxERkYiJCQEGRkZMJvNLk+aiLwLawcRNeb0kY8bb7wRFRUVSnz55ZfK3BNPPIFNmzYhLy8PhYWFOH36NCZNmuTShInIO7F2EJFCnPDcc8/JgAEDmpyrrq6WgIAAycvLU8YOHTokAKSoqMjhbVgsFgHAYDA8ICwWizMlolmsHQxG+wlH6obTRz5KS0sRGxuLbt264f7778fJkycBAMXFxbh8+TLS0tKUZVNSUpCQkICioqJm13fp0iVYrVZVEFHbw9pBRA2caj5SU1OxZs0afPbZZ1i5ciXKysowcuRI1NTUwGQyITAwEGFhYarnREdHw2QyNbvO3Nxc6PV6JeLj41v0QojIc7F2EFFj/s4sfMcddyiP+/fvj9TUVCQmJuKjjz5CUFBQixJYsGABnnzySeVvq9XKIkLUxrB2EFFj13WrbVhYGG644QYcPXoUBoMBdXV1qK6uVi1jNpthMBiaXYdOp0NoaKgqiKhtY+0gat+uq/k4d+4cjh07hpiYGAwePBgBAQEoKChQ5ktKSnDy5EkYjcbrTpSI2g7WDqJ2zuFLyUVk/vz5sm3bNikrK5OvvvpK0tLSJCoqSiorK0VEZMaMGZKQkCBbt26VXbt2idFoFKPR6MwmeMU6g+FB4aq7XVg7GIz2E47UDaeaj3vvvVdiYmIkMDBQunbtKvfee68cPXpUmb948aLMmjVLwsPDpWPHjjJx4kSpqKhgAWEwvDRc1XywdjAY7SccqRs+IiLwIFarFXq93t1pEBEAi8XiNddSsHYQeQZH6gZ/24WIiIg0xeaDiIiINMXmg4iIiDTF5oOIiIg0xeaDiIiINMXmg4iIiDTF5oOIiIg0xeaDiIiINMXmg4iIiDTF5oOIiIg0xeaDiIiINMXmg4iIiDTF5oOIiIg0xeaDiIiINMXmg4iIiDTF5oOIiIg0xeaDiIiINMXmg4iIiDTF5oOIiIg0xeaDiIiINMXmg4iIiDTF5oOIiIg0xeaDiIiINMXmg4iIiDTldPNx6tQpPPDAA4iMjERQUBD69euHXbt2KfMigkWLFiEmJgZBQUFIS0tDaWmpS5MmIu/D2kFEDZxqPn766ScMHz4cAQEB+PTTT3Hw4EG88sorCA8PV5ZZunQpVqxYgTfeeAM7duxAcHAw0tPTUVtb6/Lkicg7sHYQkYo44emnn5YRI0Y0O2+z2cRgMMiyZcuUserqatHpdPLhhx86tA2LxSIAGAyGB4TFYnGmRDSLtYPBaD/hSN1w6sjHxo0bMWTIEEyZMgVdunTBwIED8dZbbynzZWVlMJlMSEtLU8b0ej1SU1NRVFTU5DovXboEq9WqCiJqW1g7iKgxp5qP48ePY+XKlejZsyc2b96MmTNnYu7cuXj33XcBACaTCQAQHR2tel50dLQyd7Xc3Fzo9Xol4uPjW/I6iMiDsXYQkYpDxzP/V0BAgBiNRtXYnDlzZNiwYSIi8tVXXwkAOX36tGqZKVOmyD333NPkOmtra8VisShRXl7u9kNGDAbj53DVaRfWDgaj/YTLT7vExMSgT58+qrHevXvj5MmTAACDwQAAMJvNqmXMZrMydzWdTofQ0FBVEFHbwtpBRI051XwMHz4cJSUlqrEjR44gMTERAJCcnAyDwYCCggJl3mq1YseOHTAajS5Il4i8EWsHEak4c+j0m2++EX9/f/n9738vpaWl8v7770vHjh1l7dq1yjJLliyRsLAw+eSTT2Tv3r0yfvx4SU5OlosXLzq0DV6xzmB4TrjqtAtrB4PRfsKRuuFU8yEismnTJunbt6/odDpJSUmRN998UzVvs9lk4cKFEh0dLTqdTsaMGSMlJSUOr58FhMHwnHBV88HawWC0n3CkbviIiMCDWK1W6PV6d6dBRAAsFovXXEvB2kHkGRypG/xtFyIiItIUmw8iIiLSFJsPIiIi0hSbDyIiItKUxzUfHnb9K1G75k3vR2/Klagtc+S96HHNR01NjbtTIKL/5U3vR2/Klagtc+S96HG32tpsNpw+fRoigoSEBJSXl3vNrX5asVqtiI+P5765CvdL01qyX0QENTU1iI2Nha+vx31GaRJrx7Xx/dE87pumObtfnKkb/q5K0lV8fX0RFxen/Dw2f7Ohedw3TeN+aZqz+8XbvjODtcMx3C/N475pmjP7xdG64R0faYiIiKjNYPNBREREmvLY5kOn0+G5556DTqdzdyoeh/umadwvTWtv+6W9vV5Hcb80j/umaa25XzzuglMiIiJq2zz2yAcRERG1TWw+iIiISFNsPoiIiEhTbD6IiIhIU2w+iIiISFMe23y89tprSEpKQocOHZCamopvvvnG3SlpKjc3FzfffDM6deqELl26YMKECSgpKVEtU1tbi+zsbERGRiIkJAQZGRkwm81uytg9lixZAh8fH+Tk5Chj7Xm/nDp1Cg888AAiIyMRFBSEfv36YdeuXcq8iGDRokWIiYlBUFAQ0tLSUFpa6saMXYt1g3XDUawd/8ctdUM80Lp16yQwMFDeeecdOXDggDz66KMSFhYmZrPZ3alpJj09XVavXi379++XPXv2yJ133ikJCQly7tw5ZZkZM2ZIfHy8FBQUyK5du2TYsGFyyy23uDFrbX3zzTeSlJQk/fv3l3nz5inj7XW/nD17VhITEyUrK0t27Nghx48fl82bN8vRo0eVZZYsWSJ6vV42bNgg3333ndx9992SnJwsFy9edGPmrsG6wbrhKNaO/+OuuuGRzcfQoUMlOztb+bu+vl5iY2MlNzfXjVm5V2VlpQCQwsJCERGprq6WgIAAycvLU5Y5dOiQAJCioiJ3pamZmpoa6dmzp2zZskVGjRqlFJD2vF+efvppGTFiRLPzNptNDAaDLFu2TBmrrq4WnU4nH374oRYptirWDXusG/ZYO9TcVTc87rRLXV0diouLkZaWpoz5+voiLS0NRUVFbszMvSwWCwAgIiICAFBcXIzLly+r9lNKSgoSEhLaxX7Kzs7G2LFjVa8faN/7ZePGjRgyZAimTJmCLl26YODAgXjrrbeU+bKyMphMJtW+0ev1SE1N9fp9w7rRNNYNe6wdau6qGx7XfJw5cwb19fWIjo5WjUdHR8NkMrkpK/ey2WzIycnB8OHD0bdvXwCAyWRCYGAgwsLCVMu2h/20bt06fPvtt8jNzbWba8/75fjx41i5ciV69uyJzZs3Y+bMmZg7dy7effddAFBef1t8b7Fu2GPdsMfaYc9ddcO/5SmTVrKzs7F//358+eWX7k7F7crLyzFv3jxs2bIFHTp0cHc6HsVms2HIkCF46aWXAAADBw7E/v378cYbbyAzM9PN2ZHWWDfUWDua5q664XFHPqKiouDn52d3hbHZbIbBYHBTVu4ze/Zs/P3vf8cXX3yBuLg4ZdxgMKCurg7V1dWq5dv6fiouLkZlZSUGDRoEf39/+Pv7o7CwECtWrIC/vz+io6Pb5X4BgJiYGPTp00c11rt3b5w8eRIAlNffFt9brBtqrBv2WDua5q664XHNR2BgIAYPHoyCggJlzGazoaCgAEaj0Y2ZaUtEMHv2bKxfvx5bt25FcnKyan7w4MEICAhQ7aeSkhKcPHmyTe+nMWPGYN++fdizZ48SQ4YMwf333688bo/7BQCGDx9ud1vlkSNHkJiYCABITk6GwWBQ7Rur1YodO3Z4/b5h3fgZ60bzWDua5ra60eJLVVvRunXrRKfTyZo1a+TgwYPy2GOPSVhYmJhMJnenppmZM2eKXq+Xbdu2SUVFhRIXLlxQlpkxY4YkJCTI1q1bZdeuXWI0GsVoNLoxa/dofMW6SPvdL9988434+/vL73//eyktLZX3339fOnbsKGvXrlWWWbJkiYSFhcknn3wie/fulfHjx7epW21ZN1g3nMHa4b664ZHNh4jIq6++KgkJCRIYGChDhw6Vr7/+2t0paQpAk7F69WplmYsXL8qsWbMkPDxcOnbsKBMnTpSKigr3Je0mVxeQ9rxfNm3aJH379hWdTicpKSny5ptvquZtNpssXLhQoqOjRafTyZgxY6SkpMRN2boe6wbrhjNYO37mjrrhIyLS8uMmRERERM7xuGs+iIiIqG1j80FERESaYvNBREREmmLzQURERJpi80FERESaYvNBREREmmLzQURERJpi80FERESaYvNBREREmmLzQURERJpi80FERESa+v8H6uSgff+w4QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate model on radoom image\n",
    "img = test_dataset[0][3]\n",
    "with torch.no_grad():\n",
    "    rec_img = decoder(encoder(img))\n",
    "    rec_img = rec_img.squeeze().numpy()\n",
    "    img = img[:-4].squeeze().numpy().reshape(64,64)\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    ax[0].imshow(img, cmap='gray')\n",
    "    ax[0].set_title('Original image')\n",
    "    ax[1].imshow(rec_img, cmap='gray')\n",
    "    ax[1].set_title('Reconstructed image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "if save:\n",
    "    # Function to save the model\n",
    "    model_name = '64x64_MinMaxScaler'\n",
    "    def save_model(encoder, decoder, encoder_path=\"encoder.pth\", decoder_path=\"decoder.pth\"):\n",
    "        torch.save(encoder.state_dict(), encoder_path)\n",
    "        torch.save(decoder.state_dict(), decoder_path)\n",
    "        print(\"Models saved to {} and {}\".format(encoder_path, decoder_path))\n",
    "    enc = 'training_results/' + model_name + '/encoder.pth'\n",
    "    dec = 'training_results/' + model_name + '/decoder.pth'\n",
    "    save_model(encoder, decoder, encoder_path=enc, decoder_path=dec)\n",
    "    np.savetxt('training_results/' + model_name + '/train_loss.txt', diz_loss['train_loss'])\n",
    "    np.savetxt('training_results/' + model_name + '/val_loss.txt', diz_loss['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_initial_convolutions(encoder, num_filters=8, figsize=(15, 15)):\n",
    "    \"\"\"\n",
    "    Plots the initial convolutional filters of the encoder.\n",
    "    \n",
    "    Parameters:\n",
    "    - encoder: The trained encoder model.\n",
    "    - num_filters: Number of filters to plot. Default is 16.\n",
    "    - figsize: Size of the plot. Default is (15, 15).\n",
    "    \"\"\"\n",
    "    # Extract the weights from the first convolutional layer\n",
    "    conv1_weights = encoder.encoder_cnn[0].weight.data.cpu().numpy()\n",
    "    \n",
    "    # Create a figure to plot the filters\n",
    "    fig, axes = plt.subplots(1, num_filters, figsize=figsize)\n",
    "    \n",
    "    for i in range(num_filters):\n",
    "        ax = axes[i]\n",
    "        # Get the filter\n",
    "        filt = conv1_weights[i, 0, :, :]\n",
    "        # Plot the filter\n",
    "        ax.imshow(filt, cmap='gray')\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'Filter {i+1}')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Usage example\n",
    "plot_initial_convolutions(encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the latent space\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appmlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
