{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0   cluster     y     x          E   size\n",
      "0           0  004580_A   9.0  26.0   715232.0  108.0\n",
      "1           1  011701_G  15.0  17.0  1184202.0  179.0\n",
      "2           2  003882_A   3.0   3.0    31156.0    6.0\n",
      "3           3  009717_G   7.0  62.0  1423016.0  245.0\n",
      "4           4  005590_A  32.0  31.0  1014772.0  169.0\n",
      "   Unnamed: 0   cluster     y     x          E   size\n",
      "0           0  004580_A   9.0  26.0   715232.0  108.0\n",
      "1           1  011701_G  15.0  17.0  1184202.0  179.0\n",
      "2           2  003882_A   3.0   3.0    31156.0    6.0\n",
      "3           3  009717_G   7.0  62.0  1423016.0  245.0\n",
      "4           4  005590_A  32.0  31.0  1014772.0  169.0\n"
     ]
    }
   ],
   "source": [
    "meta_data = pd.read_csv('cluster_meta.csv')\n",
    "print(meta_data.head())\n",
    "\n",
    "def load_data(folder_path,meta_data):\n",
    "    data = []\n",
    "    for file in meta_data['cluster']:\n",
    "        file = file + '.csv'\n",
    "        file_path = os.path.join(folder_path,file)\n",
    "        cluster = pd.read_csv(file_path,header=None).values\n",
    "        cluster = cluster.flatten()\n",
    "        cluster = np.append(cluster, meta_data[meta_data['cluster'] == file[:-4]][['y', 'x', 'E', 'size']].values.flatten())\n",
    "        data.append(cluster)\n",
    "    combined_array = np.stack(data,axis=0)\n",
    "    print('shape of combined array: ')\n",
    "    print(combined_array.shape)\n",
    "    return combined_array\n",
    "\n",
    "\n",
    "meta_scaler = MinMaxScaler()\n",
    "meta_data_values = meta_scaler.fit_transform(meta_data[['y', 'x', 'E', 'size']])\n",
    "print(meta_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_and_pad_csvs(folder_path):\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    data_arrays = []\n",
    "    number_arrays = len(csv_files)\n",
    "    number_large = 0\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        data = pd.read_csv(file_path).values\n",
    "        data_arrays.append(data)\n",
    "    max_rows = max(array.shape[0] for array in data_arrays)\n",
    "    max_cols = max(array.shape[1] for array in data_arrays)\n",
    "    largest_array_index = max(range(len(data_arrays)), key=lambda i: data_arrays[i].shape[0] * data_arrays[i].shape[1])\n",
    "    print(f\"Index of the largest initial array: {largest_array_index}\")\n",
    "    padded_arrays = []\n",
    "    for array in data_arrays:\n",
    "        if array.shape[0] <= 64 and array.shape[1] <= 64:\n",
    "            padded_array = np.zeros((64, 64))\n",
    "            padded_array[:array.shape[0], :array.shape[1]] = array\n",
    "            padded_arrays.append(padded_array)\n",
    "            number_large += 1\n",
    "    print(f'Number of arrays that are smaller than 64x64: {number_large}, ({number_large/number_arrays*100:.2f}%)')\n",
    "    print(f'max rows: {max_rows}, max cols:{max_cols}')\n",
    "    combined_array = np.stack(padded_arrays, axis=0)\n",
    "    return combined_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of combined array: \n",
      "(3936, 4100)\n",
      "(3936, 4100)\n",
      "(3936, 4100)\n"
     ]
    }
   ],
   "source": [
    "class NumpyArrayDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data.astype(np.float32)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample  # Return only the sample and dummy label\n",
    "\n",
    "folder_path = 'clusters_colour_rotations_rescaled'\n",
    "combined_array = load_data(folder_path,meta_data)\n",
    "\n",
    "scalar = MinMaxScaler()\n",
    "combined_array = scalar.fit_transform(combined_array.reshape(-1, 1)).reshape(combined_array.shape)\n",
    "print(combined_array.shape)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "#meta_data_values = NumpyArrayDataset(meta_data_values, transform=transform)\n",
    "dataset = NumpyArrayDataset(combined_array, transform=transform)\n",
    "print(combined_array.shape)\n",
    "\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# test_size = len(dataset) - train_size\n",
    "# train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# print(meta_data_values.shape)\n",
    "\n",
    "\n",
    "# train_dataset, test_dataset, train_meta, test_meta = train_test_split(dataset, meta_data_values, test_size=0.2, random_state=42)\n",
    "# test_dataset = dataset\n",
    "\n",
    "# lazy, non-random split\n",
    "test_size = 0.2\n",
    "split_index = int(len(dataset) * (1 - test_size))\n",
    "train_dataset = dataset[:split_index]\n",
    "test_dataset = dataset[split_index:]\n",
    "train_meta = meta_data_values[:split_index]\n",
    "test_meta = meta_data_values[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3148, 4100])\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "valid_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(train_loader.dataset[0].shape)\n",
    "\n",
    "#train_meta = DataLoader(train_meta, batch_size=32, shuffle=False)\n",
    "#test_meta = DataLoader(test_meta, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.613412e+07, 3.920000e+02, 2.240000e+02, 2.400000e+02,\n",
       "        1.840000e+02, 1.440000e+02, 1.760000e+02, 2.160000e+02,\n",
       "        1.760000e+02, 3.040000e+02, 1.680000e+02, 1.680000e+02,\n",
       "        2.240000e+02, 1.120000e+02, 1.120000e+02, 1.120000e+02,\n",
       "        6.400000e+01, 5.600000e+01, 1.600000e+01, 2.400000e+01,\n",
       "        4.000000e+01, 6.400000e+01, 4.000000e+01, 1.600000e+01,\n",
       "        8.000000e+00, 0.000000e+00, 2.400000e+01, 2.400000e+01,\n",
       "        8.000000e+00, 8.000000e+00, 8.000000e+00, 1.600000e+01,\n",
       "        8.000000e+00, 0.000000e+00, 8.000000e+00, 8.000000e+00,\n",
       "        8.000000e+00, 0.000000e+00, 0.000000e+00, 8.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 1.600000e+01,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 1.600000e+01,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 8.000000e+00, 8.000000e+00,\n",
       "        8.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n",
       "        0.000000e+00, 8.000000e+00, 0.000000e+00, 8.000000e+00]),\n",
       " array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ,\n",
       "        0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21,\n",
       "        0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32,\n",
       "        0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43,\n",
       "        0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54,\n",
       "        0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65,\n",
       "        0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76,\n",
       "        0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87,\n",
       "        0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98,\n",
       "        0.99, 1.  ]),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.hist(combined_array.flatten(), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.imshow(combined_array[3929,:,:], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoded_space_dim):\n",
    "        super().__init__()\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            # Keep 3D structure\n",
    "            nn.Conv2d(1, 8, 8, stride=2, padding=1),  # 1x256x256 -> 8, 122, 122\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 16 , 16, stride=2, padding=1),  # 8, 122, 122 -> [16, 47, 47]\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),    \n",
    "            )\n",
    "        \n",
    "        # Flatten layer\n",
    "        self.flatten = nn.Flatten(start_dim=0)\n",
    " \n",
    "        # Linear section\n",
    "        self.encoder_lin = nn.Sequential(\n",
    "            nn.Linear(1296+4, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, encoded_space_dim)\n",
    "        )\n",
    "   \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        meta = x[-4:]\n",
    "        \n",
    "        #print(meta.shape)\n",
    "        x = x[:-4].unflatten(0, (1,1, 64, 64))\n",
    "        #x = x.unflatten(2, ( 64, 64))\n",
    "        # x = x[:,:,:-4].reshape(1,3096,64,64)\n",
    "        #print(x.shape)\n",
    "        x = self.encoder_cnn(x)\n",
    "        #print(\"Encoder CNN\",x.shape)\n",
    "        x = self.flatten(x)\n",
    "        # if not isinstance(meta_data, torch.Tensor):\n",
    "            # meta_data = torch.from_numpy(meta_data).to(x.device, dtype = x.dtype)\n",
    "        # print(x.shape, meta_data.shape)\n",
    "        #print(x.shape, meta.shape)\n",
    "        y = torch.cat((x, meta), dim=0)\n",
    "        #print(y.shape)\n",
    "        x = self.encoder_lin(y)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoded_space_dim):\n",
    "        super().__init__()\n",
    "        self.decoder_lin = nn.Sequential(\n",
    "            nn.Linear(encoded_space_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 1296),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        ### The decoder does the exact opposite, reconstructing the images from the latent space values.\n",
    "        self.unflatten = nn.Unflatten(dim=1, \n",
    "        unflattened_size=(16, 9, 9))\n",
    "\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(16,16,16, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16,1,8, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print('Forward pass in Decoder',x.shape)\n",
    "        x = self.decoder_lin(x).reshape((1,1296))\n",
    "        #print(\"LinNN\",x.shape)\n",
    "        x = self.unflatten(x)\n",
    "        #print(\"Unflatten\",x.shape)\n",
    "        x = self.decoder_conv(x)\n",
    "        #print(\"Conv\",x.shape)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4100])\n",
      "original data: torch.Size([1, 1, 64, 64])\n",
      "encoder:\n",
      "Conv1: torch.Size([1, 8, 30, 30])\n",
      "Conv2: torch.Size([1, 16, 9, 9])\n",
      "BatchNorm: torch.Size([1, 16, 9, 9])\n",
      "Flatten: torch.Size([1, 1296])\n",
      "Linear1: torch.Size([1, 128])\n",
      "Linear2: torch.Size([1, 10])\n",
      "decoder:\n",
      "Linear1: torch.Size([1, 128])\n",
      "Linear2: torch.Size([1, 1296])\n",
      "Unflatten: torch.Size([1, 16, 9, 9])\n",
      "Conv1: torch.Size([1, 16, 30, 30])\n",
      "BatchNorm: torch.Size([1, 16, 30, 30])\n",
      "Conv2: torch.Size([1, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# print((train_loader.dataset[0][0]))\n",
    "x = train_loader.dataset[0][0]\n",
    "print(x.shape)\n",
    "x = x[:-4].reshape((1,1,64,64))\n",
    "\n",
    "\n",
    "# encoder:\n",
    "A = nn.Conv2d(1, 8, 8, stride=2, padding=1)\n",
    "B = nn.Conv2d(8, 16 , 16, stride=2, padding=1)\n",
    "C = nn.BatchNorm2d(16)\n",
    "\n",
    "# flatten\n",
    "D = nn.Flatten()\n",
    "\n",
    "# linear\n",
    "E = nn.Linear(1296, 128)\n",
    "F = nn.Linear(128, 10)\n",
    "\n",
    "print(f'original data: {x.shape}')\n",
    "\n",
    "print('encoder:')\n",
    "\n",
    "print(f'Conv1: {A(x).shape}')\n",
    "print(f'Conv2: {B(A(x)).shape}')\n",
    "print(f'BatchNorm: {C(B(A(x))).shape}')\n",
    "print(f'Flatten: {D(C(B(A(x)))).shape}')\n",
    "print(f'Linear1: {E(D(C(B(A(x))))).shape}')\n",
    "print(f'Linear2: {F(E(D(C(B(A(x)))))).shape}')\n",
    "\n",
    "\n",
    "\n",
    "print('decoder:')\n",
    "\n",
    "\n",
    "x = train_loader.dataset[0][0]\n",
    "x = x[:-4].reshape((1,1,64,64))\n",
    "x = F(E(D(C(B(A(x))))))\n",
    "# decoder:\n",
    "G = nn.Linear(10, 128)\n",
    "H = nn.Linear(128, 1296)\n",
    "I = nn.Unflatten(dim=1, \n",
    "        unflattened_size=(16, 9, 9))\n",
    "J = nn.ConvTranspose2d(16, 16, 16, stride=2,padding = 1)#, output_padding=0)\n",
    "K = nn.BatchNorm2d(16)\n",
    "L = nn.ConvTranspose2d(16, 1, 8, stride=2, padding = 1)#, padding=1, output_padding=1)\n",
    "# M = nn.BatchNorm2d(8)\n",
    "# N = nn.ConvTranspose2d(8, 1, 3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "print(f'Linear1: {G(x).shape}')\n",
    "print(f'Linear2: {H(G(x)).shape}')\n",
    "print(f'Unflatten: {I(H(G(x))).shape}')\n",
    "print(f'Conv1: {J(I(H(G(x)))).shape}')\n",
    "print(f'BatchNorm: {K(J(I(H(G(x))))).shape}')\n",
    "print(f'Conv2: {L(K(J(I(H(G(x)))))).shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "lr = 0.001\n",
    "torch.manual_seed(42)\n",
    "\n",
    "d = 28\n",
    "encoder = Encoder(encoded_space_dim=d)\n",
    "decoder = Decoder(encoded_space_dim=d)\n",
    "params_to_optimize = [\n",
    "    {'params': encoder.parameters()},\n",
    "    {'params': decoder.parameters()}\n",
    "]\n",
    "\n",
    "optim = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-05)\n",
    "\n",
    "def train_epoch(encoder, decoder, dataloader, loss_fn, optimizer):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    train_loss = []\n",
    "    for image_batch in dataloader.dataset[0]:\n",
    "        encoded_data = encoder(image_batch)\n",
    "        decoded_data = decoder(encoded_data)\n",
    "        loss = loss_fn(decoded_data, image_batch[:-4].reshape(decoded_data.shape))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.detach().cpu().numpy())\n",
    "    return np.mean(train_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(encoder, decoder, dataloader, loss_fn):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        conc_out = []\n",
    "        conc_label = []\n",
    "        for image_batch in dataloader.dataset[0]:\n",
    "            encoded_data = encoder(image_batch)\n",
    "            decoded_data = decoder(encoded_data)\n",
    "            conc_out.append(decoded_data.cpu())\n",
    "            conc_label.append(image_batch[:-4].reshape(decoded_data.shape).cpu())\n",
    "        conc_out = torch.cat(conc_out)\n",
    "        conc_label = torch.cat(conc_label)\n",
    "        val_loss = loss_fn(conc_out, conc_label)\n",
    "    return val_loss.data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "diz_loss = {'train_loss': [], 'val_loss': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 \t train loss: 5.33468e-06 \t val loss: 4.98704e-06 \t time: 65.06310272216797\n",
      "Elapsed time: 65.06310272216797\n",
      "Epoch 2/20 \t train loss: 4.36e-06 \t Delta: 9.71258e-07 \t val loss: 3.68387e-06 \t Delta: 1.30e-06 \t time: 85.57879829406738\n",
      "Elapsed time: 85.57879829406738\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(encoder, decoder, train_loader, loss_fn, optim)\n",
    "    val_loss = test_epoch(encoder, decoder, test_loader, loss_fn)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if epoch != 0:\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs} \\t train loss: {train_loss:.2e} \\t Delta: {diz_loss['train_loss'][-1]-train_loss:.5e} \\t val loss: {val_loss:.5e} \\t Delta: {diz_loss['val_loss'][-1]-val_loss:.2e} \\t time: {elapsed_time}')\n",
    "    else:\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs} \\t train loss: {train_loss:.5e} \\t val loss: {val_loss:.5e} \\t time: {elapsed_time}')\n",
    "    print(f'Elapsed time: {elapsed_time}')\n",
    "    diz_loss['train_loss'].append(train_loss)\n",
    "    diz_loss['val_loss'].append(val_loss)\n",
    "    #plot_ae_outputs(encoder, decoder, n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_yscale('log')\n",
    "# ax.set_xscale('log')\n",
    "ax.plot(diz_loss['train_loss'], label='train loss')\n",
    "ax.plot(diz_loss['val_loss'], label='val loss')\n",
    "ax.legend()\n",
    "# ax.set_xlim(0, 50)\n",
    "\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sumf = 0\n",
    "# for i in range(6400):\n",
    "#     img = test_dataset[i][0].unsqueeze(0).numpy()\n",
    "#     suming = np.sum(img)\n",
    "#     if suming>sumf:\n",
    "#         sumf = sumimg\n",
    "#         idx = i\n",
    "# print(i)\n",
    "# print(sumf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on radoom image\n",
    "img = test_dataset[11][0].unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    rec_img = decoder(encoder(img))\n",
    "    rec_img = rec_img.squeeze().numpy()\n",
    "    img = img.squeeze().numpy()0\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    ax[0].imshow(img, cmap='gray')\n",
    "    ax[0].set_title('Original image')\n",
    "    ax[1].imshow(rec_img, cmap='gray')\n",
    "    ax[1].set_title('Reconstructed image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "if save:\n",
    "    # Function to save the model\n",
    "    model_name = '64x64_MinMaxScaler'\n",
    "    def save_model(encoder, decoder, encoder_path=\"encoder.pth\", decoder_path=\"decoder.pth\"):\n",
    "        torch.save(encoder.state_dict(), encoder_path)\n",
    "        torch.save(decoder.state_dict(), decoder_path)\n",
    "        print(\"Models saved to {} and {}\".format(encoder_path, decoder_path))\n",
    "    enc = 'training_results/' + model_name + '/encoder.pth'\n",
    "    dec = 'training_results/' + model_name + '/decoder.pth'\n",
    "    save_model(encoder, decoder, encoder_path=enc, decoder_path=dec)\n",
    "    np.savetxt('training_results/' + model_name + '/train_loss.txt', diz_loss['train_loss'])\n",
    "    np.savetxt('training_results/' + model_name + '/val_loss.txt', diz_loss['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_initial_convolutions(encoder, num_filters=8, figsize=(15, 15)):\n",
    "    \"\"\"\n",
    "    Plots the initial convolutional filters of the encoder.\n",
    "    \n",
    "    Parameters:\n",
    "    - encoder: The trained encoder model.\n",
    "    - num_filters: Number of filters to plot. Default is 16.\n",
    "    - figsize: Size of the plot. Default is (15, 15).\n",
    "    \"\"\"\n",
    "    # Extract the weights from the first convolutional layer\n",
    "    conv1_weights = encoder.encoder_cnn[0].weight.data.cpu().numpy()\n",
    "    \n",
    "    # Create a figure to plot the filters\n",
    "    fig, axes = plt.subplots(1, num_filters, figsize=figsize)\n",
    "    \n",
    "    for i in range(num_filters):\n",
    "        ax = axes[i]\n",
    "        # Get the filter\n",
    "        filt = conv1_weights[i, 0, :, :]\n",
    "        # Plot the filter\n",
    "        ax.imshow(filt, cmap='gray')\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'Filter {i+1}')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Usage example\n",
    "plot_initial_convolutions(encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the latent space\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appmlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
